#!crnn/rnn.py
# kate: syntax python;
# vim: ft=python sw=2:
# -*- mode: python -*-
# sublime: syntax 'Packages/Python Improved/PythonImproved.tmLanguage'
# vim:set expandtab tabstop=4 fenc=utf-8 ff=unix ft=python:

# via:
# /u/irie/setups/switchboard/2018-02-13--end2end-zeyer/config-train/bpe_1k.multihead-mlp-h1.red8.enc6l.encdrop03.decbs.ls01.pretrain2.nbd07.config
# Kazuki BPE1k baseline, from Interspeech paper.

from returnn.import_ import import_

import_("github.com/jotix16/returnn-experiments", "common", None)
from returnn_import.github_com.jotix16.returnn_experiments.dev.common.datasets.asr.librispeech import oggzip
from returnn_import.github_com.jotix16.returnn_experiments.dev.common.common_config import *
from returnn_import.github_com.jotix16.returnn_experiments.dev.common.models.collect_out_str import make_out_str_func
from returnn_import.github_com.jotix16.returnn_experiments.dev.common.models.encoder import blstm_cnn_specaug
from returnn_import.github_com.jotix16.returnn_experiments.dev.common.datasets.asr.timit.nltk import NltkTimit

from returnn_import.github_com.jotix16.returnn_experiments.dev.common.models.transducer.recomb_recog import targetb_recomb_recog
from returnn_import.github_com.jotix16.returnn_experiments.dev.common.models.rna_labelsync.helpers import prev_output, search_checks
from returnn_import.github_com.jotix16.returnn_experiments.dev.common.models.rna_labelsync.segmental_label_blank_distributions import segmental_blank_distribution, segmental_label_distribution
import os
import sys
import numpy
from subprocess import check_output, CalledProcessError
from returnn.tf.util.basic import DimensionTag
from returnn.config import get_global_config

config = globals()["config"]  # make PyCharm happy
task = config.value("task", "train")

globals().update(NltkTimit().get_config_opts())


# Workaround for openblas hanging:
# * https://github.com/tensorflow/tensorflow/issues/13802
# * https://github.com/rwth-i6/returnn/issues/323#issuecomment-725384762
# patch_atfork = True

# device = "gpu"
multiprocessing = True
update_on_device = True

if config.has("out_beam_size"):
  out_beam_size = config.int("out_beam_size", 0)
  print("** out_beam_size %i" % out_beam_size, file=sys.stderr)
else:
  out_beam_size = 12

if config.has("t_beam_size"):
  t_beam_size = config.int("t_beam_size", 0)
  print("** t_beam_size %i" % t_beam_size, file=sys.stderr)
else:
  t_beam_size = 100

# data
target = "classes"
target_num_labels = 61  # 1030
targetb_num_labels = target_num_labels + 1  # with blank
targetb_blank_idx = target_num_labels
# time_tag = DimensionTag(kind=DimensionTag.Types.Spatial, description="time")
# output_len_tag = DimensionTag(kind=DimensionTag.Types.Spatial, description="output-len")  # it's downsampled time
# use "same_dim_tags_as": {"t": time_tag} if same time tag ("data" and "alignment"). e.g. for RNA. not for RNN-T.
# extern_data = {
#   "data": {"dim": 40, "same_dim_tags_as": {"t": time_tag}},  # Gammatone 40-dim
#   "alignment": {"dim": targetb_num_labels, "sparse": True, "same_dim_tags_as": {"t": output_len_tag}},
#   # for cheating experiment:
#   # "target_t": {"dim": None, "sparse": True, "available_for_inference": True},
#   # "relative_t": {"dim": None, "sparse": True, "available_for_inference": False},  # TODO: not used
# }
# if task != "train":
#   # During train, we add this via the network (from prev alignment, or linear seg). Otherwise it's not available.
#   extern_data["targetb"] = {"dim": targetb_num_labels, "sparse": True, "available_for_inference": False}
#   extern_data[target] = {"dim": target_num_labels, "sparse": True}  # must not be used for chunked training
extern_data["relative_tt"] = {"dim": None, "sparse": True, "available_for_inference": False}
EpochSplit = 6


_import_baseline_setup = "ctcalign.prior0.lstm6l.withchar.lrkeyfix"
_alignment = "%s.epoch-150" % _import_baseline_setup

# train = get_sprint_dataset("train")
# dev = get_sprint_dataset("cv")
# eval_datasets = {"devtrain": get_sprint_dataset("devtrain")}

cache_size = "0"
window = 1

# Note: We control the warmup in the pretrain construction.
learning_rate = 0.001
min_learning_rate = learning_rate / 50.


def summary(name, x):
  """
  :param str name:
  :param tf.Tensor x: (batch,time,feature)
  """
  import tensorflow as tf
  # tf.summary.image wants [batch_size, height,  width, channels],
  # we have (batch, time, feature).
  img = tf.expand_dims(x, axis=3)  # (batch,time,feature,1)
  img = tf.transpose(img, [0, 2, 1, 3])  # (batch,feature,time,1)
  tf.summary.image(name, img, max_outputs=10)
  tf.summary.scalar("%s_max_abs" % name, tf.reduce_max(tf.abs(x)))
  mean = tf.reduce_mean(x)
  tf.summary.scalar("%s_mean" % name, mean)
  stddev = tf.sqrt(tf.reduce_mean(tf.square(x - mean)))
  tf.summary.scalar("%s_stddev" % name, stddev)
  tf.summary.histogram("%s_hist" % name, tf.reduce_max(tf.abs(x), axis=2))


# -- linear alignment --
def targetb_linear(source, **kwargs):
  from returnn.tf.util.basic import get_rnnt_linear_aligned_output
  enc = source(1, as_data=True, auto_convert=False)
  dec = source(0, as_data=True, auto_convert=False)
  enc_lens = enc.get_sequence_lengths()
  dec_lens = dec.get_sequence_lengths()
  out, out_lens = get_rnnt_linear_aligned_output(
    input_lens=enc_lens,
    target_lens=dec_lens, targets=dec.get_placeholder_as_batch_major(),
    blank_label_idx=targetb_blank_idx,
    targets_consume_time=True)
  return out


def targetb_linear_out(sources, **kwargs):
  from returnn.tf.util.basic import Data
  enc = sources[1].output
  # dec = sources[0].output
  size = enc.get_sequence_lengths()  # + dec.get_sequence_lengths()
  # output_len_tag.set_tag_on_size_tensor(size)
  return Data(name="targetb_linear", sparse=True, dim=targetb_num_labels, size_placeholder={0: size})
# -- -- -- -- -- -- -- --


def targetb_search_or_fallback(source, **kwargs):  # not used
  import tensorflow as tf
  from returnn.tf.util.basic import where_bc
  ts_linear = source(0)  # (B,T)
  ts_search = source(1)  # (B,T)
  la = source(2, auto_convert=False)  # (B,)
  return where_bc(tf.less(la[:, None], 0.01), ts_search, ts_linear)


def targetb_recomb_train(layer, batch_dim, scores_in, scores_base, base_beam_in, end_flags, **kwargs):  # not used
  """
  :param ChoiceLayer layer:
  :param tf.Tensor batch_dim: scalar
  :param tf.Tensor scores_base: (batch,base_beam_in,1). existing beam scores
  :param tf.Tensor scores_in: (batch,base_beam_in,dim). log prob frame distribution
  :param tf.Tensor end_flags: (batch,base_beam_in)
  :param tf.Tensor base_beam_in: int32 scalar, 1 or prev beam size
  :rtype: tf.Tensor
  :return: (batch,base_beam_in,dim), combined scores
  """
  import tensorflow as tf
  from returnn.tf.util.basic import where_bc, nd_indices, tile_transposed
  scores = scores_in + scores_base  # (batch,beam,dim)
  dim = layer.output.dim

  u = layer.explicit_search_sources[0].output  # prev:u actually. [B*beam], pos in target [0..decT-1]
  assert u.shape == ()
  u_t = tf.reshape(tf.reshape(u.placeholder, (batch_dim, -1))[:, : base_beam_in], (-1,))  # u beam might differ from base_beam_in
  targets = layer.network.parent_net.extern_data.data[target]  # BPE targets, [B,decT]
  assert targets.shape == (None,) and targets.is_batch_major
  target_lens = targets.get_sequence_lengths()  # [B]
  target_lens_exp = tile_transposed(target_lens, axis=0, multiples=base_beam_in)  # [B*beam]
  missing_targets = target_lens_exp - u_t  # [B*beam]
  allow_target = tf.greater(missing_targets, 0)  # [B*beam]
  targets_exp = tile_transposed(targets.placeholder, axis=0, multiples=base_beam_in)  # [B*beam,decT]
  targets_u = tf.gather_nd(targets_exp, indices=nd_indices(where_bc(allow_target, u_t, 0)))  # [B*beam]
  targets_u = tf.reshape(targets_u, (batch_dim, base_beam_in))  # (batch,beam)
  allow_target = tf.reshape(allow_target, (batch_dim, base_beam_in))  # (batch,beam)

  # t = layer.explicit_search_sources[1].output  # prev:t actually. [B*beam], pos in encoder [0..encT-1]
  # assert t.shape == ()
  # t_t = tf.reshape(tf.reshape(t.placeholder, (batch_dim, -1))[:,:base_beam_in], (-1,))  # t beam might differ from base_beam_in
  t_t = layer.network.get_rec_step_index() - 1  # scalar
  inputs = layer.network.parent_net.get_layer("encoder").output  # encoder, [B,encT]
  input_lens = inputs.get_sequence_lengths()  # [B]
  input_lens_exp = tile_transposed(input_lens, axis=0, multiples=base_beam_in)  # [B*beam]
  allow_blank = tf.less(missing_targets, input_lens_exp - t_t)  # [B*beam]
  allow_blank = tf.reshape(allow_blank, (batch_dim, base_beam_in))  # (batch,beam)

  dim_idxs = tf.range(dim)[None, None, :]  # (1,1,dim)
  masked_scores = where_bc(
    tf.logical_or(
      tf.logical_and(tf.equal(dim_idxs, targetb_blank_idx), allow_blank[:, :, None]),
      tf.logical_and(tf.equal(dim_idxs, targets_u[:, :, None]), allow_target[:, :, None])),
    scores, float("-inf"))

  return where_bc(end_flags[:, :, None], scores, masked_scores)


def t_recomb_recog(layer, batch_dim, scores_in, scores_base, base_beam_in, end_flags, **kwargs):  # not used
    """
    :param ChoiceLayer layer:
    :param tf.Tensor batch_dim: scalar
    :param tf.Tensor scores_base: (batch,base_beam_in,1). existing beam scores
    :param tf.Tensor scores_in: (batch,base_beam_in,dim). log prob frame distribution
    :param tf.Tensor end_flags: (batch,base_beam_in)
    :param int base_beam_in:
    :rtype: tf.Tensor
    :return: (batch,base_beam_in,dim), combined scores
    """
    import tensorflow as tf
    from returnn.tf.util.basic import where_bc
    end_flags = tf.expand_dims(end_flags, axis=-1)  # (batch,beam,1)
    scores = scores_in + scores_base  # (batch,beam,dim)
    best_idxs = tf.cast(tf.argmax(scores, axis=1), tf.int32)  # (batch,dim) -> beam idx
    mask = tf.equal(tf.range(base_beam_in)[None, :, None], best_idxs[:, None, :])  # (batch,beam,dim)
    mask2 = tf.equal(tf.range(base_beam_in)[None, :, None], base_beam_in - 1)  # keep last beam as-is for cheating
    mask = tf.logical_or(mask, mask2)
    recomb_scores = where_bc(mask, scores, float("-inf"))
    return where_bc(end_flags, scores, recomb_scores)


StoreAlignmentUpToEpoch = 10 * EpochSplit  # 0 based, exclusive
AlignmentFilenamePattern = "net-model/alignments.%i.hdf"


def get_most_recent_align_hdf_files(epoch0):  # not used
  """
  :param int epoch0: 0-based (sub) epoch
  :return: filenames or None if there is nothing completed yet
  :rtype: list[str]|None
  """
  if epoch0 < EpochSplit:
    return None
  if epoch0 > StoreAlignmentUpToEpoch:
    epoch0 = StoreAlignmentUpToEpoch  # first epoch after
  i = ((epoch0 - EpochSplit) // EpochSplit) * EpochSplit
  return [AlignmentFilenamePattern % j for j in range(i, i + EpochSplit)]


def get_learning_rate(pretrain_idx):
  lr = None
  lr_warmup = list(numpy.linspace(learning_rate * 0.1, learning_rate, num=10))
  if pretrain_idx < len(lr_warmup):
    lr = lr_warmup[pretrain_idx]
  else:
    lr = learning_rate
  return lr


def pretrain_encoder_params(pretrain_idx):
  start_num_lstm_layers = 2
  final_num_lstm_layers = 6
  num_lstm_layers = final_num_lstm_layers
  if pretrain_idx is not None:
    pretrain_idx = max(pretrain_idx, 0) // 6  # Repeat a bit.
    num_lstm_layers = pretrain_idx + start_num_lstm_layers
    pretrain_idx = num_lstm_layers - final_num_lstm_layers
    num_lstm_layers = min(num_lstm_layers, final_num_lstm_layers)

  if final_num_lstm_layers > start_num_lstm_layers:
    start_dim_factor = 0.5
    grow_frac = 1.0 - float(final_num_lstm_layers - num_lstm_layers) / (final_num_lstm_layers - start_num_lstm_layers)
    dim_frac = start_dim_factor + (1.0 - start_dim_factor) * grow_frac
  else:
    dim_frac = 1.
  time_reduction = [3, 2] if num_lstm_layers >= 3 else [6]
  return num_lstm_layers, dim_frac, time_reduction


def get_net_dict(pretrain_idx):
  """
  :param int|None pretrain_idx: starts at 0. note that this has a default repetition factor of 6
  :return: net_dict or None if pretrain should stop
  :rtype: dict[str,dict[str]|int]|None
  """
  # Note: epoch0 is 0-based here! I.e. in contrast to elsewhere, where it is 1-based.
  # Also, we never use #repetition here, such that this is correct.
  # This is important because of sub-epochs and storing the HDF files,
  # to know exactly which HDF files cover the dataset completely.
  epoch0 = pretrain_idx
  net_dict = {}

  # network params
  # EncKeyTotalDim = 200
  EncValueTotalDim = 2048
  LstmDim = EncValueTotalDim // 2
  l2 = 0.0001

  aux_ctc_loss = False  # use auxilary ctc loss
  have_existing_align = False  # only in training, and only in pretrain, and only after the first epoch
  use_targetb_search_as_target = False  # not have_existing_align or epoch0 < StoreAlignmentUpToEpoch
  keep_linear_align = False  # epoch0 is not None and epoch0 < EpochSplit * 2

  # We use this pretrain construction during the whole training time (epoch0 > num_epochs).
  if pretrain_idx is not None and epoch0 % EpochSplit == 0 and epoch0 > num_epochs:
    # Stop pretraining now.
    return None

  if pretrain_idx is not None:
    net_dict["#config"] = {}
    net_dict["#config"]["learning_rate"] = get_learning_rate(pretrain_idx)

  # We import the model, thus no growing.
  num_lstm_layers, dim_frac, time_reduction = pretrain_encoder_params(pretrain_idx)

  net_dict["#info"] = {
    "epoch0": epoch0,  # Set this here such that a new construction for every pretrain idx is enforced in all cases.
    "num_lstm_layers": num_lstm_layers,
    "dim_frac": dim_frac,
    "have_existing_align": have_existing_align,
    "use_targetb_search_as_target": use_targetb_search_as_target,
    "keep_linear_align": keep_linear_align,
  }

  # 1) Encoder provides "encoder"
  # -----------------------------
  net_dict.update({
    "encoder": blstm_cnn_specaug.make_encoder(num_layers=num_lstm_layers, time_reduction=time_reduction,
                                              lstm_dim=int(LstmDim * dim_frac), dropout=0.3 * dim_frac),
    # -- not used --
    # "enc_ctx0": {"class": "linear", "from": "encoder", "activation": None, "with_bias": False, "n_out": EncKeyTotalDim},
    # "enc_ctx_win": {"class": "window", "from": "enc_ctx0", "window_size": 5},  # [B,T,W,D]
    # "enc_val": {"class": "copy", "from": "encoder"},
    # "enc_val_win": {"class": "window", "from": "enc_val", "window_size": 5},  # [B,T,W,D]
    # -- -- -- -- --
  })

  # 2) Decoder
  # ----------
  def get_output_dict(train, search, targetb):  # TODO: targetb is not usd
    # labelsync search, see thesis
    # "t" == ts
    # "am_window": encoder[t_{s-1}+1:T]
    # "am": encoder[t_{s-1}+1]
    # "prev_out_embed": embeded(y_{s-1})
    # 1) "lm": slowRNN [B,D] <- y_{s-1}, encoder[t_{s-1}+1] == ["prev_out_non_blank", "am_window"]
    # 2) "readout": fastRNN [B, T', D] <- encoder[t_{s-1}+1] [B, T', enc_D], embeded(y_{s-1})[B, embed_D], slowRnn_output[B, D] == ["am_window", "prev_out_embed", "lm"]
    return {"class": "rec",
            "from": [],
            "back_prop": (task == "train") and train,
            "include_eos": False,

            "unit": {
              "enc_seq_len": {"class": "length", "from": "base:encoder", "sparse": False},
              "prev_t_clip": {"class": "eval",
                              "from": ["prev:t", "enc_seq_len"],
                              "eval": "tf.maximum(0, tf.minimum(source(0), source(1)-1))"},
              "am_window": {"class": "slice_nd", "from": "base:encoder", "start": "prev_t_clip", "size": None, "min_size": 1},  # [B,T',D] where T'=T-t_{s-1}+1
              "am": {"class": "gather_nd", "from": "base:encoder", "position": "prev_t_clip"},

              # ------------------------------------------------------------------------------------

              # 1) label-sync SLOWRNN
              # ---------------------
              "prev_out_non_blank": {"class": "reinterpret_data", "from": "prev:output", "set_sparse_dim": target_num_labels},  # [B]
              # This is just a normal SubnetworkLayer instead of a MaskedComputationLayer
              #                       ^ Label-sync                 ^ Time-sync
              "lm_masked":
                {"class": "subnetwork",
                 "from": "prev_out_non_blank",
                 "subnetwork": {
                   "input_embed": {"class": "linear", "from": "data", "activation": None, "with_bias": False, "n_out": 621},
                   "lstm0": {"class": "rec", "from": ["input_embed", "base:am"], "unit": "nativelstm2", "n_out": LstmDim},
                   "output": {"class": "copy", "from": "lstm0"}}
                 },  # [B, LstmDim]

              "lm": {"class": "reinterpret_data", "from": "lm_masked", "enforce_batch_major": True},  # [B, LstmDim]
              # "lm_print": {"class": "print", "from": "lm", "filename": "data/step-data/segmental/tensor.lm"},
              # ------------------------------------------------------------------------------------

              # 2) prev output
              # --------------
              # Output:
              # First decoder-step: [0 1030 1030 1030 ...] (length T)
              # After that : [prev:output 1030 1030 1030 ...] (length T')
              # We first feed "0" as the initial output, then blank-symbol
              "prev_output": {"class": "eval",
                              "from": ["prev:output", "am_window"],
                              "eval": prev_output(targetb_blank_idx=targetb_blank_idx),
                              "out_type": {"batch_dim_axis": 0, "time_dim_axis": 1, "shape": (None,),
                                           "sparse": True, "dtype": "int32"}, "n_out": targetb_num_labels},

              "prev_out_embed": {"class": "linear", "from": "prev_output", "activation": None, "n_out": 128},  # [B, T'] sparse --> [B, T', 128]
              # ------------------------------------------------------------------------------------

              # 3) FAST RNN
              # -----------                            [B, T',D]    [B,T',128]     [B, LstmDim](The T' axis will be broadcasted)
              "s_input": {"class": "copy", "from": ["am_window", "prev_out_embed", "lm"]},  # [B, T', D+128+LstmDim]
              # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
              # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
              "s": {"class": "rec",
                    "unit": "NativeLstm2StoreStates",
                    "from": "s_input",
                    "n_out": 128, "L2": l2, "dropout": 0.3,
                    "initial_state": {"c": "prev:s_state_c", "h": "prev:s_state_h"},
                    "unit_opts": {"rec_weight_dropout": 0.3}},  # [B,T',D]
              # "s_input": {"class": "print", "from": "s_input0",
              #             "filename": "data/step-data/segmental/tensor.s_input"},

              # We can't use GetLastHiddenStateLayer, because it expects the hidden state to be [B,D],
              # but we have [T',B,D] in this special case.
              "s_states_c": {"class": "eval", "from": "s", "n_out": 128,
                             "eval": "(source(0), self.sources[0].get_last_hidden_state(key='c'))[-1]",  # TODO: why not only self.sources[0].get_last_hidden_state(key='c')
                             "out_type": {"time_dim_axis": 0, "batch_dim_axis": 1, "shape": (None, 128)}},  # [T',B,D] if pulled from the loop(happens always)
              "s_states_h": {"class": "eval", "from": "s", "n_out": 128,
                             "eval": "(source(0), self.sources[0].get_last_hidden_state(key='h'))[-1]",
                             "out_type": {"time_dim_axis": 0, "batch_dim_axis": 1, "shape": (None, 128)}},  # [T',B,D]

              "s_state_c": {"class": "gather_nd", "from": "s_states_c", "position": "relative_t_clip",
                            "is_output_layer": True, "initial_output": "zeros"},
              "s_state_h": {"class": "gather_nd", "from": "s_states_h", "position": "relative_t_clip",
                            "is_output_layer": True, "initial_output": "zeros"},
              # TODO: ^^^ don't understand the rec layer and why the hidden states are pulled out expicitely?
              # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
              # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
              # "s_c": {"class": "rec",
              #         "unit": "nativelstm2",
              #         "from": "s_input",
              #         "n_out": 128, "L2": l2, "dropout": 0.3,
              #         "unit_opts": {"rec_weight_dropout": 0.3}},
              # "s_h": {"class": "rec",
              #         "unit": "nativelstm2",
              #         "from": "s_input",
              #         "n_out": 128, "L2": l2, "dropout": 0.3,
              #         "unit_opts": {"rec_weight_dropout": 0.3}},

              "s_batch": {"class": "reinterpret_data", "from": "s", "enforce_batch_major": True},  # [B, T', D]
              #                                          [B,T',128] [B, T',D]    [B, LstmDim](The T' axis will be broadcasted)
              "readout_in": {"class": "linear", "from": ["s_batch", "am_window", "lm"], "activation": None, "n_out": 1000},  # [B, T', D]
              "readout": {"class": "reduce_out", "mode": "max", "num_pieces": 2, "from": "readout_in"},  # [B, T', D/2]
              # "s_print": {"class": "print", "from": "s_batch", "filename": "data/step-data/segmental/tensor.s"},
              # "readout_in_print": {"class": "print", "from": "readout_in",
              #                      "filename": "data/step-data/segmental/tensor.readout_in"},
              # ------------------------------------------------------------------------------------

              # 4) Probabilities
              # ----------------
              "label_log_prob": {"class": "linear", "from": "readout", "activation": "log_softmax", "dropout": 0.3, "n_out": target_num_labels},  # [B, T', V]
              "label_prob": {"class": "activation", "from": "label_log_prob", "activation": "exp"},  # [B, T', V]
              # "label_log_prob_print": {"class": "print", "from": "label_log_prob",
              #                          "filename": "data/step-data/segmental/tensor.label_log_prob"},

              "emit_prob0": {"class": "linear", "from": "s_batch", "activation": None, "n_out": 1, "is_output_layer": True},  # [B, T', 1]
              "emit_log_prob": {"class": "activation", "from": "emit_prob0", "activation": "log_sigmoid"},  # [B, T', 1] emit probability  sigmoid(x)
              "blank_log_prob": {"class": "eval", "from": "emit_prob0", "eval": "tf.compat.v1.log_sigmoid(-source(0))"},  # [B, T', 1] blank probability sigmoid(-x)
              # "emit_log_prob_print": {"class": "print", "from": "emit_log_prob",
              #                         "filename": "data/step-data/segmental/tensor.emit_log_prob"},
              # ------------------------------------------------------------------------------------

              # 4) Segment boundaries(t_s) choice, beam search, keep beams
              # -----------------------------------------------------
              # for segmental model
              # p(t_s| ..)
              "relative_t_distribution": {"class": "eval",
                                          "from": ["emit_log_prob", "blank_log_prob"],
                                          "eval": segmental_blank_distribution,
                                          "out_type": {"shape": (None,), "batch_dim_axis": 0, "feature_dim_axis": 1}, "n_out": None},  # [B,T'+1] distribution over possible ts
              "relative_t": {"class": "choice",
                             "from": "relative_t_distribution", "input_type": "log_prob",
                             "initial_output": 0,
                             "cheating": False, "keep_beams": True,
                            #  "search": True,
                             "target": "relative_tt",  # TODO: not sure
                             "beam_size": t_beam_size * out_beam_size,
                             # "control_dependencies_on_output": search_checks,
                             "length_normalization": False},  # [B*beam_size] choice over distribution, keep beams
              "t": {"class": "eval", "from": ["prev:t", "relative_t"], "eval": "source(0) + source(1) + 1",
                    "initial_output": 0, "out_type": {"dtype": "int32"}},

              "t_prime": {"class": "length", "from": "am_window"},  # [B]: T'
              # we need to do this, because in the case of t_i=T+1,
              # we would gather for `alpha_t_s_label` beyond the last item. Thus we clip the relative pos.
              # also clip for the case am_window.size=0 -> size-1=-1 (clip to 0)
              "relative_t_clip": {"class": "eval", "from": ["relative_t", "t_prime"],
                                  "eval": "tf.maximum(0, tf.minimum(source(0), source(1)-1))"},
              "alpha_t_s_label": {"class": "gather_nd", "from": "label_log_prob", "position": "relative_t_clip"},  # [B,V]

              "y_s_distribution": {"class": "eval",
                                   "from": ["alpha_t_s_label", "t", "base:encoder"],
                                   "eval": segmental_label_distribution,
                                   "out_type": {"shape": (target_num_labels,), "dim": target_num_labels}},  # [B,K]
              # ------------------------------------------------------------------------------------

              # 5) Target(y_s) choice, beam search, output
              # ------------------------------------------
              'output': {  # [B*beam,1]
                'class': 'choice', 'target': target, 'beam_size': out_beam_size,
                'from': "y_s_distribution", "input_type": "log_prob",
                "initial_output": 0,
                "cheating": False,
                # "cheating": "exclusive" if task == "train" else None,
                # "control_dependencies_on_output": search_checks,
                # "explicit_search_sources": ["prev:out_str", "prev:output"] if task == "search" else None,
                # "custom_score_combine": targetb_recomb_recog if task == "search" else None
              },
              # we needed to increase the dim so that it matches the previous shapes (from time-sync)
              "output_": {"class": "reinterpret_data", "from": "output",
                          "set_sparse_dim": targetb_num_labels,
                          "initial_output": 0},  # [B*beam]


              "out_str": {
                "class": "eval",
                "from": ["prev:out_str", "output"],
                "eval": make_out_str_func(target=target),
                "initial_output": None, "out_type": {"shape": (), "dtype": "string"}},

              # "output_is_not_blank": {"class": "compare", "from": "output_", "value": targetb_blank_idx, "kind": "not_equal", "initial_output": True},
              # "output_is_diff_to_before": {"class": "compare", "from": ["output_", "prev:output_"], "kind": "not_equal"},

              # We allow repetitions of the output label. This "output_emit" is True on the first label but False otherwise, and False on blank.
              # "output_emit": {
              # "class": "eval", "from": ["output_is_not_blank", "output_is_diff_to_before"],
              # "is_output_layer": True, "initial_output": True,
              # "eval": "tf.logical_and(source(0), source(1))"},
              # "output_emit": {"class": "constant", "value": True, "initial_output": True},

              # both should be equivalent
              "end": {"class": "compare", "from": "output", "value": 0},
            },
            "target": [target],
            # "size_target": target,
            "max_seq_len": "max_len_from('encoder')",
            }

  net_dict["output"] = get_output_dict(train=True, search=(task != "train"), targetb="targetb")  # [B*beam,1] choices

  # 3) Decision and edit_distance
  # -----------------------------
  net_dict.update(
    {
      # for task "search" / search_output_layer
      # decoder provides output which is than used as following:
      "output_wo_b0": {
        "class": "masked_computation",
        "from": "output",
        "mask": "output/output_emit",
        "unit": {"class": "copy"}
      },
      "output_wo_b": {"class": "reinterpret_data", "from": "output_wo_b0", "set_sparse_dim": target_num_labels},

      "decision": {
        "class": "decide", "from": "output_wo_b", "loss": "edit_distance", "target": target,
        'only_on_search': True},
    })

  # 4) Existing alignment as extern_data TODO
  # ------------------------------------
  if have_existing_align:
    net_dict.update({
      # This should be compatible to t_linear or t_search.
      "existing_alignment": {
        "class": "reinterpret_data",
        "from": "data:alignment",
        "set_sparse": True,  # not sure what the HDF gives us
        "set_sparse_dim": targetb_num_labels,
        "size_base": "encoder",  # for RNA...
      },
    })
    net_dict.update(  # provides "targetb_base" and "targetb"
      {
        # TODO: Why does this provide targetb? We dont use "targetb" in the decoder get_output_dict(train, search, targetb)
        "targetb_linear": {
          "class": "eval", "from": ["data:%s" % target, "encoder"], "eval": targetb_linear,
          "out_type": targetb_linear_out},
        # Target for decoder ('output') with search ("extra.search") in training.
        # The layer name must be smaller than "t_target" such that this is created first.
        # Present the existing alignments as target to be used for the decoder in training
        "1_targetb_base": {
          "class": "copy",
          "from": "existing_alignment",  # if have_existing_align else "targetb_linear",
          "register_as_extern_data": "targetb_base" if task == "train" else None},

        "2_targetb_target": {
          "class": "eval",
          "from": "targetb_search_or_fallback" if use_targetb_search_as_target else "data:targetb_base",
          "eval": "source(0)",
          "register_as_extern_data": "targetb" if task == "train" else None},
      }
    )

  # 5) CTC aux loss
  # ---------------
  if aux_ctc_loss:
    net_dict.update(
      {
        "ctc_out": {"class": "softmax", "from": "encoder", "with_bias": False, "n_out": targetb_num_labels},
        # "ctc_out_prior": {"class": "reduce", "mode": "mean", "axes": "bt", "from": "ctc_out"},
        # log-likelihood: combine out + prior
        "ctc_out_scores": {
          "class": "eval", "from": ["ctc_out"],
          "eval": "safe_log(source(0))",
          # "eval": "safe_log(source(0)) * am_scale - tf.stop_gradient(safe_log(source(1)) * prior_scale)",
          # "eval_locals": {
          #   "am_scale": 1.0,  # WrapEpochValue(lambda epoch: numpy.clip(0.05 * epoch, 0.1, 0.3)),
          #   "prior_scale": 0.5  # WrapEpochValue(lambda epoch: 0.5 * numpy.clip(0.05 * epoch, 0.1, 0.3))
          # }
        },

        "3_target_masked": {
          "class": "reinterpret_data", "from": "output_wo_b0",
          "set_sparse_dim": target_num_labels,  # we masked blank away
          "enforce_batch_major": True,  # ctc not implemented otherwise...
          "register_as_extern_data": "targetb_masked" if task == "train" else None},

        # TODO: CTC loss between the transducer alignments and ctc alignments?? why not use real targets here?
        "ctc": {"class": "copy",
                "from": "ctc_out_scores",
                "loss": "ctc" if task == "train" else None,
                "target": "targetb_masked" if task == "train" else None,
                "loss_opts": {
                  "beam_width": 1, "use_native": True, "output_in_log_space": True,
                  "ctc_opts": {"logits_normalize": False}} if task == "train" else None
                },
        # "ctc_align": {"class": "forced_align", "from": "ctc_out_scores", "input_type": "log_prob",
        # "align_target": "data:%s" % target, "topology": "ctc"},
      }
    )

  return net_dict


network = get_net_dict(pretrain_idx=None)
search_output_layer = "decision"
debug_print_layer_output_template = True


# import_model_train_epoch1 = "base/data-train/base2.conv2l.specaug4a/net-model/network.160"
_train_setup_dir = "data-train/rna3c-lm4a.convtrain.switchout6.l2a_1e_4.nohdf.encbottle256.attwb5_am.dec1la-n128.decdrop03.decwdrop03.pretrain_less2_rep6.mlr50.emit2.fl2.fixmask.rna-align-blank0-scratch-swap.encctc.devtrain.retrain1"
model = _train_setup_dir + "/net-model/network"
# preload_from_files = {
#   "base": {
#     "init_for_train": True,
#     "ignore_missing": True,
#     "filename": "/u/zeyer/setups/switchboard/2018-10-02--e2e-bpe1k/data-train/base2.conv2l.specaug4a/net-model/network.160",
#   },
#   "encoder": {
#     "init_for_train": True,
#     "ignore_missing": True,
#     "filename": "/u/zeyer/setups/switchboard/2017-12-11--returnn/data-train/#dropout01.l2_1e_2.6l.n500.inpstddev3.fl2.max_seqs100.grad_noise03.nadam.lr05e_3.nbm6.nbrl.grad_clip_inf.nbm3.run1/net-model/network.077",
#   },
#   "encoder": {
#     "init_for_train": True,
#     "ignore_missing": True,
#     "ignore_params_prefixes": {"output/"},
#     "filename": "/u/zeyer/setups/switchboard/2019-10-22--e2e-bpe1k/data-train/%s/net-model/network.pretrain.250" % _import_baseline_setup,
#   }
# }
# lm_model_filename = "/work/asr3/irie/experiments/lm/switchboard/2018-01-23--lmbpe-zeyer/data-train/bpe1k_clean_i256_m2048_m2048.sgd_b16_lr0_cl2.newbobabs.d0.2/net-model/network.023"

# debug_print_layer_output_shape = True

# trainer
batching = "random"
# Seq-length 'data' Stats:
#  37867 seqs
#  Mean: 447.397258827
#  Std dev: 350.353162012
#  Min/max: 15 / 2103
# Seq-length 'bpe' Stats:
#  37867 seqs
#  Mean: 14.1077719386
#  Std dev: 13.3402518828
#  Min/max: 2 / 82
log_batch_size = True
batch_size = 10000
max_seqs = 200
# max_seq_length = {"classes": 75}
_time_red = 6
_chunk_size = 60
chunking = ({"data": _chunk_size * _time_red,
             "alignment": _chunk_size
             },
            {"data": _chunk_size * _time_red // 2,
             "alignment": _chunk_size // 2,
             })
# chunking_variance ...
# min_chunk_size ...


def custom_construction_algo(idx, net_dict):
  # For debugging, use: python3 ./crnn/Pretrain.py config...
  return get_net_dict(pretrain_idx=idx)

# No repetitions here. We explicitly do that in the construction.
# pretrain = {"copy_param_mode": "subset", "construction_algo": custom_construction_algo}

# import_model_train_epoch1 = "base/data-train/rna3c-lm4a.convtrain.switchout6.l2a_1e_4.nohdf.encbottle256.dec1la-n128.decdrop03.decwdrop03.pretrain_less2_rep6.mlr50.emit2.fl2.rep.fixmask.ctcalignfix-ctcalign-p0-6l.chunk60.encctc.devtrain/net-model/network.pretrain.149"


num_epochs = 150
# model = "net-model/network"
cleanup_old_models = True
gradient_clip = 0
# gradient_clip_global_norm = 1.0
adam = True
optimizer_epsilon = 1e-8
accum_grad_multiple_step = 2
# debug_add_check_numerics_ops = True
# debug_add_check_numerics_on_output = True
stop_on_nonfinite_train_score = False
tf_log_memory_usage = True
gradient_noise = 0.0
# lr set above
learning_rate_control = "newbob_multi_epoch"
learning_rate_control_error_measure = "dev_error_output/output_prob"
learning_rate_control_relative_error_relative_lr = True
learning_rate_control_min_num_epochs_per_new_lr = 3
use_learning_rate_control_always = True
newbob_multi_num_epochs = 6
newbob_multi_update_interval = 1
newbob_learning_rate_decay = 0.7
learning_rate_file = "newbob.data"

# log
# log = "| /u/zeyer/dotfiles/system-tools/bin/mt-cat.py >> log/crnn.seq-train.%s.log" % task
log = "log/crnn.%s.log" % task
log_verbosity = 5
