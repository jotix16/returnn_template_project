#!crnn/rnn.py
# kate: syntax python;
# vim: ft=python sw=2:
# based on Andre Merboldt rnnt-fs.bpe1k.readout.zoneout.lm-embed256.lr1e_3.no-curric.bs12k.mgpu.retrain1.config
from __future__ import annotations
from typing import Dict, Any, Optional, List
from returnn.import_ import import_
from returnn_import.github_com.jotix16.returnn_experiments.dev.common.models.encoder import blstm_cnn_specaug

import_("github.com/jotix16/returnn-experiments", "common", None)
from returnn_import.github_com.jotix16.returnn_experiments.dev.common.common_config import *
from returnn_import.github_com.jotix16.returnn_experiments.dev.common.datasets.asr.librispeech import oggzip
from returnn_import.github_com.jotix16.returnn_experiments.dev.common.models.transducer.transducer_fullsum import make_net
from returnn_import.github_com.jotix16.returnn_experiments.dev.common.training.pretrain import Pretrain

from returnn_import.github_com.jotix16.returnn_experiments.dev.common.models.transducer.transducer_fullsum import (
  LayerRef, LayerDict, _base,
  Net,
  Context,
  _IMaker,
  DecoderLogProbSeparateNb,
  DecoderLogProbSeparateWb,
  IDecoderSlowRnn,
  IDecoderFastRnn,
  IDecoderLogProbSeparateNb,
  IDecoderLogProbSeparateWb,
  DecoderSlowRnnLstmIndependent,
  DecoderFastRnnOnlyReadout)
from returnn_import.github_com.jotix16.returnn_experiments.dev.common.models.encoder import blstm_cnn_specaug
from returnn_import.github_com.jotix16.returnn_experiments.dev.common.models.collect_out_str import make_out_str_func
from returnn_import.github_com.jotix16.returnn_experiments.dev.common.datasets.interface import TargetConfig
from returnn_import.github_com.jotix16.returnn_experiments.dev.common.models.transducer.recomb_recog import targetb_recomb_recog
from returnn_import.github_com.jotix16.returnn_experiments.dev.common.models.transducer.loss import (
  rnnt_loss,
  rnnt_loss_out_type,
  rna_tf_loss,
  rna_loss_out_type)


# data
globals().update(
  oggzip.Librispeech(train_random_permute={
    "rnd_scale_lower": 1., "rnd_scale_upper": 1.,
    "rnd_pitch_switch": 0.05,
    "rnd_stretch_switch": 0.05,
    "rnd_zoom_switch": 0.5,
    "rnd_zoom_order": 0,
  }).get_config_opts())


class Decoder(_IMaker):
  def __init__(self, *,
               slow_rnn: IDecoderSlowRnn = None,
               fast_rnn: IDecoderFastRnn = None,
               log_prob_separate_nb: IDecoderLogProbSeparateNb = None,
               log_prob_separate_wb: IDecoderLogProbSeparateWb = None,
               **kwargs):
    super().__init__(**kwargs)
    if not slow_rnn:
      slow_rnn = DecoderSlowRnnLstmIndependent(ctx=self.ctx)
    self.slow_rnn = slow_rnn
    if not fast_rnn:
      fast_rnn = DecoderFastRnnOnlyReadout(ctx=self.ctx)
    self.fast_rnn = fast_rnn
    if not log_prob_separate_nb:
      log_prob_separate_nb = DecoderLogProbSeparateNb(ctx=self.ctx)
    self.log_prob_separate_nb = log_prob_separate_nb
    if not log_prob_separate_wb:
      log_prob_separate_wb = DecoderLogProbSeparateWb(ctx=self.ctx)
    self.log_prob_separate_wb = log_prob_separate_wb

  def make(self, encoder: LayerRef):
    target = self.ctx.target
    beam_size = self.ctx.beam_size
    train = self.ctx.train
    search = self.ctx.search
    blank_idx = self.ctx.blank_idx

    rec_decoder = {
      "am0": {"class": "gather_nd", "from": _base(encoder), "position": "prev:t"},  # [B,D]
      "am": {"class": "copy", "from": "am0" if search else "data:source"},

      "prev_output_wo_b": {
        "class": "masked_computation", "unit": {"class": "copy", "initial_output": 0},
        "from": "prev:output_", "mask": "prev:output_emit", "initial_output": 0},
      "prev_out_non_blank": {  # N_nb
        "class": "reinterpret_data", "from": "prev_output_wo_b", "set_sparse_dim": target.get_num_classes()},

      "slow_rnn": self.slow_rnn.make(  # N_nb
        prev_sparse_label_nb="prev_out_non_blank",  # will be embeded before being used should be N_nb
        prev_emit="prev:output_emit",
        unmasked_sparse_label_nb_seq=None if search else "lm_input",  # might enable optimization if used
        prev_fast_rnn="prev:fast_rnn",
        encoder="am"),

      "fast_rnn": self.fast_rnn.make(
        prev_label_wb="prev:output_",
        slow_rnn="slow_rnn",
        encoder="am"),

      "output_log_prob_nb": self.log_prob_separate_nb.make(fast_rnn="fast_rnn"),  # N_nb
      "output_log_prob_wb": self.log_prob_separate_wb.make(fast_rnn="fast_rnn", log_prob_nb="output_log_prob_nb"),  # N_wb

      "output": {
        "class": 'choice',
        'target': f"{target.key}0" if train else None,  # note: wrong! but this is ignored both in full-sum training and in search
        'beam_size': beam_size,
        'from': "output_log_prob_wb", "input_type": "log_prob",
        "initial_output": 0,
        "length_normalization": False,
        "cheating": "exclusive" if train else None,  # only relevant for train+search
        "explicit_search_sources": ["prev:out_str", "prev:output"] if search and targetb_recomb_recog else None,
        "custom_score_combine": targetb_recomb_recog if search else None
      },
      "output_": {"class": "copy", "from": "output", "initial_output": 0},

      "out_str": {
        "class": "eval", "from": ["prev:out_str", "output_emit", "output"],
        "initial_output": None, "out_type": {"shape": (), "dtype": "string"},
        "eval": make_out_str_func(target=target.key)},

      "output_is_not_blank": {
        "class": "compare", "from": "output_", "value": blank_idx,
        "kind": "not_equal", "initial_output": True},

      # initial state=True so that we are consistent to the training and the initial state is correctly set.
      "output_emit": {"class": "copy", "from": "output_is_not_blank", "is_output_layer": True, "initial_output": True},

      "const0": {"class": "constant", "value": 0, "collocate_with": ["du", "dt", "t", "u"], "dtype": "int32"},
      "const1": {"class": "constant", "value": 1, "collocate_with": ["du", "dt", "t", "u"], "dtype": "int32"},

      # pos in target, [B]
      "du": {"class": "switch", "condition": "output_emit", "true_from": "const1", "false_from": "const0"},
      "u": {"class": "combine", "from": ["prev:u", "du"], "kind": "add", "initial_output": 0},

      # pos in input, [B]
      # output label: stay in t, otherwise advance t (encoder)
      "dt": {"class": "switch", "condition": "output_is_not_blank", "true_from": "const0", "false_from": "const1"},
      "t": {"class": "combine", "from": ["dt", "prev:t"], "kind": "add", "initial_output": 0},
      "t1": {"class": "switch", "condition": "end", "true_from": "enc_seq_len_1", "false_from": "t"},

      # stop at U+T
      # in recog: stop when all input has been consumed
      # in train: defined by target.
      "enc_seq_len": {"class": "length", "from": f"base:{encoder}", "sparse": False},
      "enc_seq_len_1": {"class": "combine", "from": ["enc_seq_len", "const1"], "kind": "sub", "initial_output": 0},
      "end": {"class": "compare", "from": ["t", "enc_seq_len"], "kind": "equal"},
    }

    if train:
      rec_decoder["full_sum_loss"] = {
        "class": "eval",
        "from": ["output_log_prob_wb", f"base:data:{target.key}", f"base:{encoder}"],
        "eval": rna_tf_loss,
        "out_type": rna_loss_out_type,
        "loss": "as_is",
      }

    if not search:
      rec_decoder["lm_input"] = {"class": "prefix_in_time", "from": f"base:data:{target.key}", "prefix": 0}

    return {
      "class": "rec",
      # In training, go framewise over the input, and inside the loop, we build up the whole 2D space (TxS).
      "from": [] if search else encoder,
      "include_eos": True,
      "back_prop": train,
      "unit": rec_decoder,
      "max_seq_len": f"max_len_from('base:{encoder}')*3",
    }
