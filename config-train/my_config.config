#!returnn.py
# -*- mode: python -*-
# vim:set expandtab tabstop=4 fenc=utf-8 ff=unix ft=python:

import os
import numpy
from subprocess import check_output, CalledProcessError
# from TFUtil import DimensionTag, Data
from returnn.tf.util.data import DimensionTag, Data
import pathlib
import sys
from returnn.config import get_global_config

# Environment
use_tensorflow = True
device = "gpu"
multiprocessing = True
update_on_device = True
cache_size = "0"
window = 1

#############################################################################
############ A. Global Required For Dataset- & Network- Creation ############
"""
Global Parameters Required:
    task, _target_num_labels, _targetb_num_labels, _targetb_blank_idx, _target, beam_size, model
Global Functions Required:
    rna_loss(), targetb_recomb_recog(), switchout_target(), out_str(), rna_fullsum_alignment()
"""
config = get_global_config()
task = config.value("task", "train")
model = "net-model/network"

# Learning Rate Control. Note: We control the warmup in the pretrain construction.
learning_rate = 0.001
min_learning_rate = learning_rate / 50.

# Beam Size
if config.has("beam_size"):
  beam_size = config.int("beam_size", 0)
  print("** beam_size %i" % beam_size)
else:
  if task == "train":
    beam_size = 4
  else:
    beam_size = 12

# Dataset informations
EpochSplit = 6
# SwitchBoard
# _target = "bpe"
# _target_num_labels = 1030
# _targetb_num_labels = _target_num_labels + 1  # with blank
# _targetb_blank_idx = _target_num_labels

# LibriSpeech
_target = "classes"
_target_num_labels = 1056
_targetb_num_labels = _target_num_labels + 1  # with blank
_targetb_blank_idx = _target_num_labels


#############################################################################

def summary(name, x):
  """
  :param str name:
  :param tf.Tensor x: (batch,time,feature)
  """
  from TFCompat import v1 as tf
  # tf.summary.image wants [batch_size, height,  width, channels],
  # we have (batch, time, feature).
  img = tf.expand_dims(x, axis=3)  # (batch,time,feature,1)
  img = tf.transpose(img, [0, 2, 1, 3])  # (batch,feature,time,1)
  tf.summary.image(name, img, max_outputs=10)
  tf.summary.scalar("%s_max_abs" % name, tf.reduce_max(tf.abs(x)))
  mean = tf.reduce_mean(x)
  tf.summary.scalar("%s_mean" % name, mean)
  stddev = tf.sqrt(tf.reduce_mean(tf.square(x - mean)))
  tf.summary.scalar("%s_stddev" % name, stddev)
  tf.summary.histogram("%s_hist" % name, tf.reduce_max(tf.abs(x), axis=2))


def _mask(x, batch_axis, axis, pos, max_amount, mask_value=0.):
  """
  :param tf.Tensor x: (batch,time,[feature])
  :param int batch_axis:
  :param int axis:
  :param tf.Tensor pos: (batch,)
  :param int|tf.Tensor max_amount: inclusive
  :param float|int mask_value:
  """
  # import tensorflow as tf
  # from TFCompat import v1 as tf
  # from TFUtil import where_bc
  from returnn.tf.compat import v1 as tf
  from returnn.tf.util.basic import where_bc
  ndim = x.get_shape().ndims
  n_batch = tf.shape(x)[batch_axis]
  dim = tf.shape(x)[axis]
  amount = tf.random_uniform(shape=(n_batch,), minval=1, maxval=max_amount + 1, dtype=tf.int32)
  pos2 = tf.minimum(pos + amount, dim)
  idxs = tf.expand_dims(tf.range(0, dim), 0)  # (1,dim)
  pos_bc = tf.expand_dims(pos, 1)  # (batch,1)
  pos2_bc = tf.expand_dims(pos2, 1)  # (batch,1)
  cond = tf.logical_and(tf.greater_equal(idxs, pos_bc), tf.less(idxs, pos2_bc))  # (batch,dim)
  if batch_axis > axis:
    cond = tf.transpose(cond)  # (dim,batch)
  cond = tf.reshape(cond, [tf.shape(x)[i] if i in (batch_axis, axis) else 1 for i in range(ndim)])
  x = where_bc(cond, mask_value, x)
  return x


def random_mask(x, batch_axis, axis, min_num, max_num, max_dims, mask_value=0.):
  """
  :param tf.Tensor x: (batch,time,feature)
  :param int batch_axis:
  :param int axis:
  :param int|tf.Tensor min_num:
  :param int|tf.Tensor max_num: inclusive
  :param int|tf.Tensor max_dims: inclusive
  :param float|int mask_value:
  """
  # import tensorflow as tf
  # from TFCompat import v1 as tf
  from returnn.tf.compat import v1 as tf
  n_batch = tf.shape(x)[batch_axis]
  if isinstance(min_num, int) and isinstance(max_num, int) and min_num == max_num:
    num = min_num
  else:
    num = tf.random_uniform(shape=(n_batch,), minval=min_num, maxval=max_num + 1, dtype=tf.int32)
  # https://github.com/tensorflow/tensorflow/issues/9260
  # https://timvieira.github.io/blog/post/2014/08/01/gumbel-max-trick-and-weighted-reservoir-sampling/
  z = -tf.log(-tf.log(tf.random_uniform((n_batch, tf.shape(x)[axis]), 0, 1)))
  _, indices = tf.nn.top_k(z, num if isinstance(num, int) else tf.reduce_max(num))
  # indices should be sorted, and of shape (batch,num), entries (int32) in [0,dim)
  # indices = tf.Print(indices, ["indices", indices, tf.shape(indices)])
  if isinstance(num, int):
    for i in range(num):
      x = _mask(x, batch_axis=batch_axis, axis=axis, pos=indices[:, i], max_amount=max_dims, mask_value=mask_value)
  else:
    _, x = tf.while_loop(
      cond=lambda i, _: tf.less(i, tf.reduce_max(num)),
      body=lambda i, x: (
        i + 1,
        tf.where(
          tf.less(i, num),
          _mask(x, batch_axis=batch_axis, axis=axis, pos=indices[:, i], max_amount=max_dims, mask_value=mask_value),
          x)),
      loop_vars=(0, x))
  return x


def transform(source, **kwargs):
  from returnn.tf.compat import v1 as tf
  data = source(0, as_data=True)
  time_factor = 1  # for switchout == 6
  x = data.placeholder
  network = kwargs["self"].network

  step = network.global_train_step
  step1 = tf.where(tf.greater_equal(step, 1000), 1, 0)
  step2 = tf.where(tf.greater_equal(step, 2000), 1, 0)

  def get_masked():
    x_masked = x
    x_masked = random_mask(
      x_masked, batch_axis=data.batch_dim_axis, axis=data.time_dim_axis,
      min_num=step1 + step2, max_num=tf.maximum(tf.shape(x)[data.time_dim_axis] // 100, 2) * (1 + step1 + step2 * 2),
      max_dims=20 // time_factor)
    x_masked = random_mask(
      x_masked, batch_axis=data.batch_dim_axis, axis=data.feature_dim_axis,
      min_num=step1 + step2, max_num=2 + step1 + step2 * 2,
      max_dims=data.dim // 5)
    return x_masked

  """
  Uses get_masked() for train or {return x} for eval based on self.train_flag.
  It will be a branched evaluation.
  """
  x = network.cond_on_train(get_masked, lambda: x)
  return x


def switchout_target(self, source, **kwargs):
  # import tensorflow as tf
  # from TFCompat import v1 as tf
  # from TFUtil import where_bc
  from returnn.tf.compat import v1 as tf
  from returnn.tf.util.basic import where_bc

  network = self.network
  time_factor = 6
  data = source(0, as_data=True)
  assert data.is_batch_major  # just not implemented otherwise
  x = data.placeholder

  def get_switched():
    # 0.05 probability to switch
    # if switch:
    # 0.5 probability to switch to blank,
    # 0.5 probability to switch to a random label
    x_ = x
    shape = tf.shape(x)
    n_batch = tf.shape(x)[data.batch_dim_axis]
    n_time = tf.shape(x)[data.time_dim_axis]
    take_rnd_mask = tf.less(tf.random_uniform(shape=shape, minval=0., maxval=1.), 0.05)
    take_blank_mask = tf.less(tf.random_uniform(shape=shape, minval=0., maxval=1.), 0.5)
    rnd_label = tf.random_uniform(shape=shape, minval=0, maxval=_target_num_labels, dtype=tf.int32)
    rnd_label = where_bc(take_blank_mask, _targetb_blank_idx, rnd_label)
    x_ = where_bc(take_rnd_mask, rnd_label, x_)
    x_ = random_mask(  # ??
      x_, batch_axis=data.batch_dim_axis, axis=data.time_dim_axis,
      min_num=0, max_num=tf.maximum(tf.shape(x)[data.time_dim_axis] // (50 // time_factor), 1),
      max_dims=20 // time_factor,
      mask_value=_targetb_blank_idx)
    # x_ = tf.Print(x_, ["switch", x[0], "to", x_[0]], summarize=100)
    return x_

  x = network.cond_on_train(get_switched, lambda: x)
  return x


# NOT USED replaced by targetb_recomb_recog()
def targetb_recomb_train(layer, batch_dim, scores_in, scores_base, base_beam_in, end_flags, **kwargs):
  """
  :param ChoiceLayer layer:
  :param tf.Tensor batch_dim: scalar
  :param tf.Tensor scores_base: (batch,base_beam_in,1). existing beam scores
  :param tf.Tensor scores_in: (batch,base_beam_in,dim). log prob frame distribution
  :param tf.Tensor end_flags: (batch,base_beam_in)
  :param tf.Tensor base_beam_in: int32 scalar, 1 or prev beam size
  :rtype: tf.Tensor
  :return: (batch,base_beam_in,dim), combined scores
  """
  # import tensorflow as tf
  # from TFCompat import v1 as tf
  # from TFUtil import where_bc, nd_indices, tile_transposed
  from returnn.tf.compat import v1 as tf
  from returnn.tf.util.basic import where_bc, nd_indices, tile_transposed

  scores = scores_in + scores_base  # (batch,beam,dim)
  dim = layer.output.dim

  u = layer.explicit_search_sources[0].output  # prev:u actually. [B*beam], pos in target [0..decT-1]
  assert u.shape == ()
  u_t = tf.reshape(tf.reshape(u.placeholder, (batch_dim, -1))[:, :base_beam_in],
    (-1,))  # u beam might differ from base_beam_in
  targets = layer.network.parent_net.extern_data.data[target]  # BPE targets, [B,decT]
  assert targets.shape == (None,) and targets.is_batch_major
  target_lens = targets.get_sequence_lengths()  # [B]
  target_lens_exp = tile_transposed(target_lens, axis=0, multiples=base_beam_in)  # [B*beam]
  missing_targets = target_lens_exp - u_t  # [B*beam]
  allow_target = tf.greater(missing_targets, 0)  # [B*beam]
  targets_exp = tile_transposed(targets.placeholder, axis=0, multiples=base_beam_in)  # [B*beam,decT]
  targets_u = tf.gather_nd(targets_exp, indices=nd_indices(where_bc(allow_target, u_t, 0)))  # [B*beam]
  targets_u = tf.reshape(targets_u, (batch_dim, base_beam_in))  # (batch,beam)
  allow_target = tf.reshape(allow_target, (batch_dim, base_beam_in))  # (batch,beam)

  # t = layer.explicit_search_sources[1].output  # prev:t actually. [B*beam], pos in encoder [0..encT-1]
  # assert t.shape == ()
  # t_t = tf.reshape(tf.reshape(t.placeholder, (batch_dim, -1))[:,:base_beam_in], (-1,))  # t beam might differ from base_beam_in
  t_t = layer.network.get_rec_step_index() - 1  # scalar
  inputs = layer.network.parent_net.get_layer("encoder").output  # encoder, [B,encT]
  input_lens = inputs.get_sequence_lengths()  # [B]
  input_lens_exp = tile_transposed(input_lens, axis=0, multiples=base_beam_in)  # [B*beam]
  allow_blank = tf.less(missing_targets, input_lens_exp - t_t)  # [B*beam]
  allow_blank = tf.reshape(allow_blank, (batch_dim, base_beam_in))  # (batch,beam)

  dim_idxs = tf.range(dim)[None, None, :]  # (1,1,dim)
  masked_scores = where_bc(
    tf.logical_or(
      tf.logical_and(tf.equal(dim_idxs, _targetb_blank_idx), allow_blank[:, :, None]),
      tf.logical_and(tf.equal(dim_idxs, targets_u[:, :, None]), allow_target[:, :, None])),
    scores, float("-inf"))

  return where_bc(end_flags[:, :, None], scores, masked_scores)


def get_vocab_tf():
  from returnn.datasets.generating import Vocabulary
  from returnn.tf.util.basic import get_shared_vocab
  from returnn.tf.compat import v1 as tf
  # from GeneratingDataset import Vocabulary
  # from TFUtil import get_shared_vocab
  # import tensorflow as tf
  # from TFCompat import v1 as tf

  dataset = Dataset.get_librispeech_dataset("train")
  vocab = Vocabulary.create_vocab(**dataset["bpe"])
  labels = vocab.labels  # bpe labels ("@@" at end, or not), excluding blank
  labels = [(l + " ").replace("@@ ", "") for l in labels] + [""]
  labels_t = get_shared_vocab(labels)
  return labels_t


def get_vocab_sym(i):
  """
  :param tf.Tensor i: e.g. [B], int32
  :return: same shape as input, string
  :rtype: tf.Tensor
  """
  # import tensorflow as tf
  # from TFCompat import v1 as tf
  from returnn.tf.compat import v1 as tf
  return tf.gather(params=get_vocab_tf(), indices=i)


def out_str(source, **kwargs):
  # ["prev:out_str", "output_emit", "output"]
  # import tensorflow as tf
  # from TFCompat import v1 as tf
  # from TFUtil import where_bc
  from returnn.tf.compat import v1 as tf
  from returnn.tf.util.basic import where_bc
  return source(0) + where_bc(source(1), get_vocab_sym(source(2)), tf.constant(""))


def get_filtered_score_op(verbose=False):
  cpp_code = """
    #include "tensorflow/core/framework/op.h"
    #include "tensorflow/core/framework/op_kernel.h"
    #include "tensorflow/core/framework/shape_inference.h"
    #include "tensorflow/core/framework/resource_mgr.h"
    #include "tensorflow/core/framework/resource_op_kernel.h"
    #include "tensorflow/core/framework/tensor.h"
    #include "tensorflow/core/platform/macros.h"
    #include "tensorflow/core/platform/mutex.h"
    #include "tensorflow/core/platform/types.h"
    #include "tensorflow/core/public/version.h"
    #include <cmath>
    #include <map>
    #include <set>
    #include <string>
    #include <tuple>

    using namespace tensorflow;

    REGISTER_OP("GetFilteredScore")
    .Input("prev_str: string")
    .Input("scores: float32")
    .Input("labels: string")
    .Output("new_scores: float32")
    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {
        c->set_output(0, c->input(1));
        return Status::OK();
    });

    class GetFilteredScoreOp : public OpKernel {
    public:
    using OpKernel::OpKernel;
    void Compute(OpKernelContext* context) override {
        const Tensor* prev_str = &context->input(0);
        const Tensor* scores = &context->input(1);
        const Tensor* labels = &context->input(2);

        int n_batch = prev_str->shape().dim_size(0);
        int n_beam = prev_str->shape().dim_size(1);

        Tensor* ret;
        OP_REQUIRES_OK(context, context->allocate_output(0, TensorShape({n_batch, n_beam}), &ret));
        for(int bat = 0; bat < n_batch; ++bat)
            for(int hyp = 0; hyp < n_beam; ++hyp)
                ret->tensor<float, 2>()(bat, hyp) = scores->tensor<float, 2>()(bat, hyp);

        for(int bat = 0; bat < n_batch; ++bat) {
            std::map<tstring, std::set<int> > new_hyps;  // seq -> set of hyp idx

            for(int hyp = 0; hyp < n_beam; ++hyp) {
                auto& seq_set = new_hyps[prev_str->tensor<tstring, 2>()(bat, hyp)];
                seq_set.insert(hyp);
            }

            for(const auto& items : new_hyps) {
                if(std::get<1>(items).size() > 1) {
                    float best_score = 0.;
                    int best_idx = -1;
                    for(int idx : std::get<1>(items)) {
                        float score = scores->tensor<float, 2>()(bat, idx);
                        if(score > best_score || best_idx == -1) {
                            best_score = score;
                            best_idx = idx;
                        }
                    }

                    float sum_score = 0.;
                    for(int idx : std::get<1>(items)) {
                        float score = scores->tensor<float, 2>()(bat, idx);
                        sum_score += expf(score - best_score);
                    }
                    sum_score = logf(sum_score) + best_score;

                    for(int idx : std::get<1>(items)) {
                        if(idx != best_idx)
                            ret->tensor<float, 2>()(bat, idx) = -std::numeric_limits<float>::infinity();
                        else
                            ret->tensor<float, 2>()(bat, idx) = sum_score;
                    }
                }
            }
        }
    }
    };
    REGISTER_KERNEL_BUILDER(Name("GetFilteredScore").Device(DEVICE_CPU), GetFilteredScoreOp);
    """
  from TFUtil import OpCodeCompiler
  compiler = OpCodeCompiler(
    base_name="GetFilteredScore", code_version=1, code=cpp_code,
    is_cpp=True, use_cuda_if_available=False, verbose=verbose)
  tf_mod = compiler.load_tf_module()
  return tf_mod.get_filtered_score


def get_filtered_score_cpp(prev_str, scores, labels):
  """
  :param tf.Tensor prev_str: (batch,beam)
  :param tf.Tensor scores: (batch,beam)
  :param list[bytes] labels: len (dim)
  :return: scores with logsumexp at best, others -inf, (batch,beam)
  :rtype: tf.Tensor
  """
  # from TFUtil import get_shared_vocab
  # from TFCompat import v1 as tf
  from returnn.tf.compat import v1 as tf
  from returnn.tf.util.basic import get_shared_vocab

  with tf.device("/cpu:0"):
    labels_t = get_shared_vocab(labels)
    return get_filtered_score_op()(prev_str, scores, labels_t)


def targetb_recomb_recog(layer, batch_dim, scores_in, scores_base, base_beam_in, end_flags, **kwargs):
  """
  :param ChoiceLayer layer:
  :param tf.Tensor batch_dim: scalar
  :param tf.Tensor scores_base: (batch,base_beam_in,1). existing beam scores
  :param tf.Tensor scores_in: (batch,base_beam_in,dim). log prob frame distribution
  :param tf.Tensor end_flags: (batch,base_beam_in)
  :param tf.Tensor base_beam_in: int32 scalar, 1 or prev beam size
  :rtype: tf.Tensor
  :return: (batch,base_beam_in,dim), combined scores
  """
  # from TFCompat import v1 as tf
  # from TFUtil import where_bc, nd_indices, tile_transposed
  from returnn.tf.compat import v1 as tf
  from returnn.tf.util.basic import where_bc, nd_indices, tile_transposed
  from returnn.datasets.generating import Vocabulary

  dim = layer.output.dim

  prev_str = layer.explicit_search_sources[0].output  # [B*beam], str
  prev_str_t = tf.reshape(prev_str.placeholder, (batch_dim, -1))[:, :base_beam_in]
  prev_out = layer.explicit_search_sources[1].output  # [B*beam], int32
  prev_out_t = tf.reshape(prev_out.placeholder, (batch_dim, -1))[:, :base_beam_in]

  dataset = Dataset.get_librispeech_dataset("train")
  vocab = Vocabulary.create_vocab(**dataset["bpe"])
  labels = vocab.labels  # bpe labels ("@@" at end, or not), excluding blank
  labels = [(l + " ").replace("@@ ", "").encode("utf8") for l in labels] + [b""]

  # Pre-filter approx (should be much faster), sum approx (better).
  scores_base = tf.reshape(
    get_filtered_score_cpp(prev_str_t, tf.reshape(scores_base, (batch_dim, base_beam_in)), labels),
    (batch_dim, base_beam_in, 1))

  scores = scores_in + scores_base  # (batch,beam,dim)

  # Mask -> max approx, in all possible options, slow.
  # mask = get_score_mask_cpp(prev_str_t, prev_out_t, scores, labels)
  # masked_scores = where_bc(mask, scores, float("-inf"))
  # Sum approx in all possible options, slow.
  # masked_scores = get_new_score_cpp(prev_str_t, prev_out_t, scores, labels)

  # scores = where_bc(end_flags[:,:,None], scores, masked_scores)

  return scores


def rna_loss(source, **kwargs):
  """
  Computes the RNA loss function.

  :param log_prob:
  :return:
  """
  # acts: (B, T, U, V)
  # targets: (B, U-1)
  # input_lengths (B,)
  # label_lengths (B,)
  import sys
  sys.path.insert(0, os.path.join(os.path.dirname(os.path.abspath(__file__)), "code"))

  # from TFCompat import v1 as tf
  from returnn.tf.compat import v1 as tf
  log_probs = source(0, as_data=True, auto_convert=False).get_placeholder_as_batch_major()
  targets = source(1, as_data=True, auto_convert=False)
  encoder = source(2, as_data=True, auto_convert=False)

  enc_lens = encoder.get_sequence_lengths()
  dec_lens = targets.get_sequence_lengths()

  from rna_tf_impl import tf_forward_shifted_rna
  costs = -tf_forward_shifted_rna(log_probs, targets.get_placeholder_as_batch_major(), enc_lens, dec_lens,
    blank_index=_targetb_blank_idx, debug=False)
  costs = tf.where(tf.is_finite(costs), costs, tf.zeros_like(costs))
  return costs


def rna_fullsum_alignment(source, **kwargs):
  """
  Computes the RNA loss function. Used only to create alignments.
  :inputs output_log_prob, real_target, "base:encoder"
  :param log_prob:
  :return: alignments: [B, T] for each frame a value in [0:blank_ix]
  """
  # acts: (B, T, U, V)
  # targets: (B, U-1)
  # input_lengths (B,):
  # label_lengths (B,)
  import sys
  sys.path.insert(0, os.path.join(os.path.dirname(os.path.abspath(__file__)), "code"))

  # import tensorflow as tf
  # from TFUtil import get_shape_dim, check_input_dim
  # from TFCompat import v1 as tf
  from returnn.tf.util.basic import get_shape_dim, check_input_dim
  from returnn.tf.compat import v1 as tf

  log_probs = source(0, as_data=True, auto_convert=False).get_placeholder_as_batch_major()
  targets = source(1, as_data=True, auto_convert=False)
  encoder = source(2, as_data=True, auto_convert=False)

  enc_lens = encoder.get_sequence_lengths()
  dec_lens = targets.get_sequence_lengths()

  target_len = get_shape_dim(targets.get_placeholder_as_batch_major(), 1)
  log_probs = check_input_dim(log_probs, 2, target_len + 1)

  from rna_tf_impl import tf_forward_shifted_rna
  costs, alignment = tf_forward_shifted_rna(log_probs, targets.get_placeholder_as_batch_major(), enc_lens, dec_lens,
    blank_index=_targetb_blank_idx, debug=False, with_alignment=True)
  return alignment  # (B, T)


debug_mode = False
if int(os.environ.get("DEBUG", "0")):
  # print("** DEBUG MODE")
  debug_mode = True

_cf_cache = {}


def cf(filename):
  """Cache manager"""
  if filename in _cf_cache:
    return _cf_cache[filename]
  if debug_mode or check_output(["hostname"]).strip().decode("utf8") in ["cluster-cn-211", "sulfid"]:
    print("use local file: %s" % filename)
    return filename  # for debugging
  try:
    cached_fn = check_output(["cf", filename]).strip().decode("utf8")
  except CalledProcessError:
    print("Cache manager: Error occured, using local file")
    return filename
  assert os.path.exists(cached_fn)
  _cf_cache[filename] = cached_fn
  return cached_fn


class Dataset():
  ## File paths
  estimated_num_seqs = {"train": 227047, "cv": 3000, "devtrain": 3000}  # wc -l segment-file
  sprint_bpe_file = '/work/asr3/irie/data/switchboard/subword_clean/ready/swbd_clean.bpe_code_1k'  # merges made in the according order(helps with encoding)
  sprint_vocab_file = '/work/asr3/irie/data/switchboard/subword_clean/ready/vocab.swbd_clean.bpe_code_1k'  # the bpe token list(size of output 1030)

  @staticmethod
  def _get_dataset_files(task):
    assert task in {"train", "devtrain", "cv", "dev", "hub5e_01", "rt03s"}
    corpus_name = {"cv": "train", "devtrain": "train"}.get(task, task)  # train, dev, hub5e_01, rt03s, original_dev

    # see /u/tuske/work/ASR/switchboard/corpus/readme
    # and zoltans mail https://mail.google.com/mail/u/0/#inbox/152891802cbb2b40
    files = {}
    files["config"] = "config/training.config"
    files["corpus"] = "/work/asr3/irie/data/switchboard/corpora/%s.corpus.gz" % corpus_name
    files["features"] = "/u/tuske/work/ASR/switchboard/feature.extraction/gt40_40/data/gt.%s.bundle" % corpus_name
    if task in {"train", "cv", "devtrain"}:
      files["segments"] = "dependencies/seg_%s" % {"train": "train", "cv": "cv_head3000", "devtrain": "train_head3000"}[
        task]

    for k, v in sorted(files.items()):
      assert os.path.exists(v), "%s %r does not exist" % (k, v)
    return files

  @classmethod
  def get_sprint_dataset(cls, task):
    epoch_split = EpochSplit if task == "train" else 1
    files = cls._get_dataset_files(task)

    args = [
      "--config=" + files["config"],
      lambda: "--*.corpus.file=" + cf(files["corpus"]),
      lambda: "--*.corpus.segments.file=" + (cf(files["segments"]) if "segments" in files else ""),
      lambda: "--*.feature-cache-path=" + cf(files["features"]),
      "--*.log-channel.file=/dev/null",
      "--*.window-size=1",
      "--*.corpus.segment-order-shuffle=true",
      "--*.segment-order-sort-by-time-length=true",
      "--*.segment-order-sort-by-time-length-chunk-size=%i" % (epoch_split * 1000 if (task == "train") else -1),
    ]

    return {
      "class": "ExternSprintDataset",
      "sprintTrainerExecPath": "sprint-executables/nn-trainer",
      "sprintConfigStr": args,
      "suppress_load_seqs_print": True,  # less verbose
      "partition_epoch": epoch_split,
      "estimated_num_seqs": (cls.estimated_num_seqs[task] // epoch_split) if task in cls.estimated_num_seqs else None,
      "input_stddev": 3.,
      "bpe": {
        'bpe_file': cls.sprint_bpe_file,
        'vocab_file': cls.sprint_vocab_file,  ## whyy do we need this?
        # 'seq_postfix': [0]  # no EOS needed for RNN-T/RNA
      }
    }

  @classmethod
  def get_aligned_hdf_dataset(cls, task, hdf_file):
    assert task in {"train", "cv", "devtrain"}  # otherwise it doesnt make sense
    ## 1. Sprint Dataset Configuration
    files = cls._get_dataset_files(task)
    args = [
      "--config=" + files["config"],
      lambda: "--*.corpus.file=" + cf(files["corpus"]),
      lambda: "--*.corpus.segments.file=" + cf(files["segments"]),
      lambda: "--*.feature-cache-path=" + cf(files["features"]),
      "--*.log-channel.file=/dev/null",
      "--*.window-size=1"
    ]
    d = {
      "class": "ExternSprintDataset",
      "sprintTrainerExecPath": "sprint-executables/nn-trainer",
      "sprintConfigStr": args,
      "suppress_load_seqs_print": True,  # less verbose
      "input_stddev": 3.,
      "bpe": {
        'bpe_file': cls.sprint_bpe_file,
        'vocab_file': cls.sprint_vocab_file,
        # 'seq_postfix': [0]  # no EOS needed for RNN-T/RNA
      }
    }

    ## 2. HDF Dataset Configuration
    epoch_split = EpochSplit if task == "train" else 1
    align_opts = {
      "class": "HDFDataset", "files": [hdf_file],
      "use_cache_manager": True,
      "seq_list_filter_file": files["segments"],  # otherwise not right selection
      # "unique_seq_tags": True  # dev set can exist multiple times
      "partition_epoch": epoch_split,
      "estimated_num_seqs": (cls.estimated_num_seqs[task] // epoch_split) if task in cls.estimated_num_seqs else None,
      "seq_ordering": "laplace:%i" % (cls.estimated_num_seqs[task] // 1000) if task == "train" else None,
      "seq_order_seq_lens_file": "/u/zeyer/setups/switchboard/dataset/data/seq-lens.train.txt.gz" if task == "train" else None
    }

    return {
      "class": "MetaDataset",
      "datasets": {"sprint": d, "align": align_opts},
      "data_map": {
        "data": ("sprint", "data"),
        "alignment": ("align", "data"),
      },
      "seq_order_control_dataset": "align",  # it must support get_all_tags
    }

  @staticmethod
  def get_extern_data(task, target_num_labels, targetb_num_labels):
    _time_tag = DimensionTag(kind=DimensionTag.Types.Spatial, description="time")
    _output_len_tag = DimensionTag(kind=DimensionTag.Types.Spatial, description="output-len")  # it's downsampled time
    # use "same_dim_tags_as": {"t": time_tag} if same time tag ("data" and "alignment"). e.g. for RNA. not for RNN-T.
    extern_data = {
      "data": {"dim": 40, "same_dim_tags_as": {"t": _time_tag}},  # Gammatone 40-dim
      "alignment": {"dim": _targetb_num_labels, "sparse": True, "same_dim_tags_as": {"t": _output_len_tag}},
      _target: {"dim": _target_num_labels, "sparse": True},  # see vocab
      # During train we dont require it, we get it from the dataset.
      # Otherwise it's not available, we add this via the network (from prev alignment, or linear seg).
      "targetb": {"dim": _targetb_num_labels, "sparse": True,
        "available_for_inference": False} if task != "train" else None
      # "align_score": {"shape": (1,), "dtype": "float32"},
    }
    return extern_data

  # LibriSpeech
  @staticmethod
  def get_librispeech_dataset(key, subset=None, train_partition_epoch=None):
    from pathlib import Path
    dataset_path = os.path.join(Path(os.path.dirname(os.path.abspath(__file__))).parent.absolute(), "base/dataset_symlink")
    path_to_Dataset = dataset_path
    path_to_zipped_libri = "%s/ogg-zips" % dataset_path
    d = {
      'class': 'LibriSpeechCorpus',
      'path': path_to_zipped_libri,
      "use_zip": True,
      "use_ogg": True,
      # "use_cache_manager": not debug_mode,
      "prefix": key,
      "bpe": {
        'bpe_file': '%s/trans.bpe_1000.codes' % path_to_Dataset,
        'vocab_file': '%s/trans.bpe_1000.vocab' % path_to_Dataset,
        # 'seq_postfix': [0],  # not needed for RNA/RNN-T
        'unknown_label': '<unk>'},
      "audio": {
        "norm_mean": '%s/stats.mean.txt' % path_to_Dataset,
        "norm_std_dev": '%s/stats.std_dev.txt' % path_to_Dataset},
    }
    if key.startswith("train"):
      d["partition_epoch"] = train_partition_epoch
      if key == "train":
        d["epoch_wise_filter"] = {
          (1, 20): {
            'use_new_filter': True,
            'subdirs': ['train-clean-100', 'train-clean-360']},
        }
      # d["audio"]["random_permute"] = True
      num_seqs = 281241  # total ??
      d["seq_ordering"] = "laplace:%i" % (num_seqs // 1000)
    else:
      d["fixed_random_seed"] = 1
      d["seq_ordering"] = "sorted_reverse"
    if subset:
      d["fixed_random_subset"] = subset  # faster
    return d

  @staticmethod
  def get_librispeech_extern_data(task, target_num_labels, targetb_num_labels):
    _time_tag = DimensionTag(kind=DimensionTag.Types.Spatial, description="time")
    _output_len_tag = DimensionTag(kind=DimensionTag.Types.Spatial, description="output-len")  # it's downsampled time
    extern_data = {
      "data": {"dim": 40, "same_dim_tags_as": {"t": _time_tag}},  # Gammatone 40-dim
      _target: {"dim": target_num_labels, "sparse": True},  # see vocab
      "alignment": {"dim": targetb_num_labels, "sparse": True, "same_dim_tags_as": {"t": _output_len_tag}},
    }
    if task != "train":
      extern_data["targetb"] = {"dim": targetb_num_labels, "sparse": True, "available_for_inference": False}
    return extern_data


class Network():
  """
  Global Parameters Required:
      task, _target_num_labels, _targetb_num_labels, _targetb_blank_idx, _target, beam_size, model
      rna_loss(), targetb_recomb_recog(), switchout_target(), out_str(), rna_fullsum_alignment()
  """

  @classmethod
  def _get_network(cls, target: str, full_sum_loss: bool = False, full_sum_alignment: bool = False,
      ce_loss: bool = False,
      pretrain_frac: float = 1, grow_encoder: bool = True):

    full_sum = full_sum_loss or full_sum_alignment

    EncKeyTotalDim = 200
    EncValueTotalDim = 2048
    LstmDim = EncValueTotalDim // 2
    l2 = 0.0001
    # Attention
    AttNumHeads = 1
    AttentionDropout = 0.1
    EncKeyPerHeadDim = EncKeyTotalDim // AttNumHeads

    # INPUT
    def get_input_dict():
      """
      Provides
              "source0": input data of dim [B, T,40,1]
      """
      return {
        "source": {"class": "eval", "eval": transform},  # SpecAugment
        "source0": {"class": "split_dims", "axis": "F", "dims": (-1, 1), "from": "source"},  # [T,40,1]
      }

    # ENCODER
    def get_encoder_dict():
      """
      Requires
              "source0": input feature vectors
      Provides: values and keys used to calculate attention context
              "encoder": encoders output [B,T,D] == [B,T,(256 or 2*LstmDim*dim_frac if full_sum) ]
              "enc_val_win": encoder value window [B,T,W,D] == [B,T,5,(256 or 2*LstmDim*dim_frac if full_sum) ]
              "enc_ctx_win": encoder value keys [B,T,W,D] == [B,T,5,200]
      """

      def get_conv_stacl_dict():
        return {
          # Lingvo: ep.conv_filter_shapes = [(3, 3, 1, 32), (3, 3, 32, 32)],  ep.conv_filter_strides = [(2, 2), (2, 2)]
          "conv0": {"class": "conv", "from": "source0", "padding": "same", "filter_size": (3, 3), "n_out": 32,
            "activation": None, "with_bias": True},  # (T,40,32)
          "conv0p": {"class": "pool", "mode": "max", "padding": "same", "pool_size": (1, 2), "from": "conv0"},
          # (T,20,32) because stride=pool_size per default
          "conv1": {"class": "conv", "from": "conv0p", "padding": "same", "filter_size": (3, 3), "n_out": 32,
            "activation": None, "with_bias": True},  # (T,20,32)
          "conv1p": {"class": "pool", "mode": "max", "padding": "same", "pool_size": (1, 2), "from": "conv1"},
          # (T,10,32)
          "conv_merged": {"class": "merge_dims", "from": "conv1p", "axes": "static"}  # (T, 320)
        }

      def get_blstm_stack_dict():
        """
        Requires
            "conv_merged": output of conv feature extractor (T, 320)
        Provides
            "encoder_0": output of encoder (T, 2*LstmDim*dim_frac)

        Creates the encoder BLST stack according to the pretraining factor
        - According to pretrain_frac
                - nr of encoder layers is increased with factor pre_train_frac from 2 -> 6 (2 + (6-2)*pre_train_frac)
                - dimension of the layers is inctreased based on dim_frac which goes from 0.5 -> 1 [0.5 + 0.5*(nr_layers_now-2)/4 == 0.5 +0.5*pre_train_fac]
                  and so taking values from 512->1028  where 1028 == EncValueTotalDim //2
        - Before each additional BLSTM layer max pooling with same padding is performed
                - if we have only 1 additional BLSTM the pool size is 6
                - otherwise the size of pool windows are 3, 2, 1, 1, 1 (i.e. the last 3 BLSTMs use a pool window of size 1)
        - Beside the first BLST all additional BLSTMs have a droput of 0.3*dim_frac(so for the last BLSTM we have droput=0.3)
        - If grow_encoder is False --> we use the max nr of layers and max dim_size
        """
        start_num_lstm_layers = 2
        final_num_lstm_layers = 6
        start_dim_factor = 0.5
        if grow_encoder:
          num_lstm_layers = start_num_lstm_layers + int((final_num_lstm_layers - start_num_lstm_layers) * pretrain_frac)
          grow_frac = 1.0 - float(final_num_lstm_layers - num_lstm_layers) / (
              final_num_lstm_layers - start_num_lstm_layers)
          dim_frac = start_dim_factor + (1.0 - start_dim_factor) * grow_frac
        else:
          num_lstm_layers = final_num_lstm_layers
          dim_frac = 1.
        time_reduction = [3, 2] if num_lstm_layers >= 3 else [6]
        blsm_stack = {}
        src = "conv_merged"  # (T, 320)
        if num_lstm_layers >= 1:
          blsm_stack.update({
            "lstm0_fw": {"class": "rec", "unit": "nativelstm2", "n_out": int(LstmDim * dim_frac), "L2": l2,
              "direction": 1, "from": src, "trainable": True},
            "lstm0_bw": {"class": "rec", "unit": "nativelstm2", "n_out": int(LstmDim * dim_frac), "L2": l2,
              "direction": -1, "from": src, "trainable": True}})
          src = ["lstm0_fw", "lstm0_bw"]
        for i in range(1, num_lstm_layers):
          red = time_reduction[i - 1] if (i - 1) < len(time_reduction) else 1
          blsm_stack.update({
            "lstm%i_pool" % (i - 1): {"class": "pool", "mode": "max", "padding": "same", "pool_size": (red,),
              "from": src}})
          src = "lstm%i_pool" % (i - 1)
          blsm_stack.update({
            "lstm%i_fw" % i: {"class": "rec", "unit": "nativelstm2", "n_out": int(LstmDim * dim_frac), "L2": l2,
              "direction": 1, "from": src, "dropout": 0.3 * dim_frac, "trainable": True},
            "lstm%i_bw" % i: {"class": "rec", "unit": "nativelstm2", "n_out": int(LstmDim * dim_frac), "L2": l2,
              "direction": -1, "from": src, "dropout": 0.3 * dim_frac, "trainable": True}})
          src = ["lstm%i_fw" % i, "lstm%i_bw" % i]
        blsm_stack["encoder0"] = {"class": "copy", "from": src}  # dim: EncValueTotalDim (T, LstmDim*dim_frac*2)
        return blsm_stack

      def get_encoder_outputs_dict():
        return {
          "encoder": {"class": "copy", "from": "encoder0"} if full_sum  # no bottleneck for fullsum
          else {"class": "linear", "from": "encoder0", "n_out": 256, "activation": None},
          "enc_ctx0": {
            "class": "linear", "from": "encoder",
            "activation": None, "with_bias": False, "n_out": EncKeyTotalDim,
            "dropout": 0.2 if full_sum else None,
            "L2": l2 if full_sum else None
          },  # [B, T, EncKeyTotalDim=200]
          "enc_ctx_win": {"class": "window", "from": "enc_ctx0", "window_size": 5},  # [B,T,W,D] == [B,T,5,200]
          "enc_val": {"class": "copy", "from": "encoder"},
          "enc_val_win": {"class": "window", "from": "enc_val", "window_size": 5},
          # [B,T,W,D] == [B,T,5,(256 or 2*LstmDim*dim_frac if full_sum) ]
          "enc_seq_len": {"class": "length", "from": "encoder", "sparse": True},  # only for debugg maybe, not used
        }

      encoder_dict = {}
      encoder_dict.update(get_conv_stacl_dict())
      encoder_dict.update(get_blstm_stack_dict())
      encoder_dict.update(get_encoder_outputs_dict())
      return encoder_dict

    # LM
    def get_lm_inputs_dict():
      """
      Provides the target sequence with blank prepended
      Used only for full_sum training otherwise forced alignements are used
      Provides
              "lm_input": LM input
      Requires
              "data:target": target part of the (input,target) training dataset
      """
      return {
        "lm_input0": {"class": "copy", "from": "data:%s" % target},
        "lm_input1": {"class": "prefix_in_time", "from": "lm_input0", "prefix": _targetb_blank_idx},
        # adds blank in the beginning of target sequence
        "lm_input": {"class": "copy", "from": "lm_input1"}
      }

    # CTC
    def get_ctc_dict():
      """
      Not used for full_sum only for CE
      Takes the output of the encoder and that of decoder and calculates ctc loss. ??
      """

      # 1.
      def get_masked_output_dict():
        """
        Provides
                "targetb_masked": masked output required for ctc(mask away the blank from decoders output)
        Requires
                "output/output_emit":
                "output": output of the decoder
        """
        return {
          "_target_masked": {
            "class": "masked_computation",
            "mask": "output/output_emit",
            "from": "output",
            "unit": {"class": "copy"}},
          "3_target_masked": {
            "class": "reinterpret_data", "from": "_target_masked",
            "set_sparse_dim": _target_num_labels,  # we masked blank away
            "enforce_batch_major": True,  # ctc not implemented otherwise...
            "register_as_extern_data": "targetb_masked" if task == "train" else None}
        }

      # 2.
      def get_ctc_outputs_dict():
        """
        Provides
                "ctc": ctc loss
        Requires
                "encoder": encoder's output"
                "targetb_masked": masked output
        """
        return {
          "ctc_out": {"class": "softmax", "from": "encoder", "with_bias": False, "n_out": _targetb_num_labels},
          "ctc_out_scores": {"class": "eval", "from": ["ctc_out"], "eval": "safe_log(source(0))"},
          "ctc":
            {
              "class": "copy", "from": "ctc_out_scores",
              "loss": "ctc" if task == "train" else None,
              "target": "targetb_masked" if task == "train" else None,
              # "targetb_masked" from below in get_masked_output()
              "loss_opts":
                {
                  "beam_width": 1, "use_native": True, "output_in_log_space": True,
                  "ctc_opts": {"logits_normalize": False}
                } if task == "train" else None
            }
        }

      ctc_dict = {}
      ctc_dict.update(get_masked_output_dict())
      ctc_dict.update(get_ctc_outputs_dict())
      return ctc_dict

    # DECISION
    def get_output_decision_dict():
      # for task "search" / search_output_layer
      """
      Requires
              "output": output of the Decoder
              "output_emit": True if last emit was not blank. Starts with True.
      Provides
              "output_wo_b": output without the blank
              "decision": decision according to edit_distance loss ?? this is used for evaluation right?
      """
      return {
        "output_wo_b0": {
          "class": "masked_computation",
          "unit": {"class": "copy"},
          "from": "output",
          "mask": "output/output_emit"},
        "output_wo_b": {"class": "reinterpret_data", "from": "output_wo_b0", "set_sparse_dim": _target_num_labels},
        # make sure it has the dim if output_emit is false
        "decision": {"class": "decide", "from": "output_wo_b", "loss": "edit_distance", "target": _target,
          'only_on_search': True},
      }

    # DECODER(slowrnn + fastrnn + beamsearch)
    def get_output_dict(train, search, target, beam_size=beam_size):
      # 1. Attention
      def get_attention_dict():
        """
        SelfAttention on Encoders Output
        Provides
                "att": attention of "am"
        Requires
                "am": query
                "base:enc_ctx_win": keys from encoder
                "base:enc_val_win": values from encoder
        """
        # 1. ATTENTION.. could make more efficient...
        return {
          "enc_ctx_win": {"class": "gather_nd", "from": "base:enc_ctx_win", "position": ":i"},
          # [B,W,D==EncKeyTotalDim] keys
          "enc_val_win": {"class": "gather_nd", "from": "base:enc_val_win", "position": ":i"},  # [B,W,D2] values
          "att_query": {"class": "linear", "from": "am", "activation": None, "with_bias": False,
            "n_out": EncKeyTotalDim},  # [B,T,D==EncKeyTotalDim] query
          "att_energy": {"class": "dot", "red1": "f", "red2": "f", "var1": "static:0", "var2": None,
            "from": ['enc_ctx_win', 'att_query']},  # (B, W)
          "att_weights0": {"class": "softmax_over_spatial", "axis": "static:0", "from": 'att_energy',
            "energy_factor": EncKeyPerHeadDim ** -0.5},  # (B, W)
          "att_weights1": {"class": "dropout", "dropout_noise_shape": {"*": None}, "from": 'att_weights0',
            "dropout": AttentionDropout},
          "att_weights": {"class": "merge_dims", "from": "att_weights1", "axes": "except_time"},
          "att": {"class": "dot", "from": ['att_weights', 'enc_val_win'], "red1": "static:0", "red2": "static:0",
            "var1": None, "var2": "f"},  # (B, V)
        }

      # 2. SlowRnn
      def get_decoder_slow_rnn_dict():
        """
        SLOW RNN
        Uses last output(true target if train and fs) and attention(if not full_sum) and calculates
        a new SlowRNN LSTM state if the last output was not blank. This state is then used in FastRNN.
        Provides
                "lm": SlowRnn output
        Requires
                "prev:output_": the last output. In case of training, after switchout (sparse)
                "prev:output_emit": True if last emit was not blank. Starts with True. (sparse)
                "base:lm_input": LM input (sparse)
                "base:att": Attention (EncValueTotalDim=2048)
        """
        lm_dict = {
          "prev_out_non_blank": {"class": "reinterpret_data", "from": "prev:output_",
            "set_sparse_dim": _target_num_labels},
          "lm_masked": {
            "class": "masked_computation",
            "mask": "prev:output_emit",  # True if last emit was not blank. Starts with True.
            "from": "prev_out_non_blank",  # last output(after switchout)
            "masked_from": "base:lm_input" if train and full_sum else None,
            "unit": {  # enables optimization if used
              "class": "subnetwork", "from": "data",  # data: output_/lm_input  (i.e. last true target)
              "subnetwork": {
                "input_embed": {"class": "linear", "activation": None, "from": "data", "with_bias": False,
                  "n_out": 256 if full_sum else 621,
                  "L2": l2 if full_sum else None},
                "lstm0": {"class": "rec", "unit": "nativelstm2", "n_out": LstmDim,
                  "from": "input_embed" if full_sum else ["input_embed", "base:att"],
                  "dropout": 0.2 if full_sum else None,
                  "L2": l2 if full_sum else None},
                "output": {"class": "copy", "from": "lstm0"}}}}}
        # Output
        if full_sum:
          # we need it masked here, so that it is (B, T, U+1, V) and not (B, T, U, V)
          lm_dict["lm"] = {"class": "copy", "from": "lm_masked"}  # [B,L] # For full-sum we need lm_masked
        else:
          lm_dict.update({
            "lm_embed_masked": {"class": "copy", "from": "lm_masked"},
            "lm_embed_unmask": {"class": "unmask", "from": "lm_embed_masked", "mask": "prev:output_emit"},
            "lm": {"class": "copy", "from": "lm_embed_unmask"}
          }
          )
        return lm_dict

      # 3. FastRnn
      def get_decoder_fast_rnn_cell_dict():
        """
        A.K.A Fast RNN
        Requires
                "am": encoder
                "prev:output_": the last output. In case of training, after switchout// only for ce
                "lm": output of SlowRnn
                "att": attention
        Provides
                "s": State of LSTM cell
                "label_log_prob": Log Distribution over vocab w/o blank
        Debug
                "label_prob": Distribution over vocab w/o blank [i.e. exp("label_log_prob") ]
        """

        def get_lstm_state_dict():
          if full_sum:
            # for full-sum: "s" is simply a copy of "am"
            return {"s": {"class": "copy", "from": "am"}}
          else:
            return {"prev_out_embed": {"class": "linear", "from": "prev:output_", "activation": None, "n_out": 128},
              "s": {"class": "rec", "unit": "nativelstm2",
                "from": ["am", "prev_out_embed", "lm"], "n_out": 128,
                "L2": l2, "dropout": 0.3, "unit_opts": {"rec_weight_dropout": 0.3}}
            }

        # 1. STATE ( for fs is only a copy of am)
        lstm_cell_dict = get_lstm_state_dict()
        # 2. Output Probabilities
        lstm_cell_dict.update({
          "joint": {"class": "copy", "from": ["s", "att", "lm"]},
          "readout_in": {"class": "linear", "from": "joint", "activation": None, "n_out": 1000,
            "dropout": 0.2 if full_sum else None,
            "L2": l2 if full_sum else None},  # extra normaliz for full-sum
          "readout": {"class": "reduce_out", "mode": "max", "num_pieces": 2, "from": "readout_in"},
          "label_log_prob": {"class": "linear", "from": "readout", "activation": "log_softmax",
            "n_out": _target_num_labels, "dropout": 0.3},
          "label_prob": {"class": "activation", "from": "label_log_prob", "activation": "exp",
            "n_out": _target_num_labels},  # for debug
        }
        )
        return lstm_cell_dict

      # 4.
      def get_vocab_and_blank_distro_dict():
        """
        Requires
                "label_log_prob": Log Distribution over vocab w/o blank
                "s": State of LSTM Cell
        Provides
                "output_log_prob": the log distribution over the whole vocab inkl blank
        """
        voc_dict = {
          ## 1. Emit Probability (i.e. 1-blank_prob)
          "emit_prob0": {"class": "linear", "from": "s", "activation": None, "n_out": 1, "is_output_layer": True},
          "emit_log_prob": {"class": "activation", "from": "emit_prob0", "activation": "log_sigmoid"},
          ## 2. Blank log prob(dirac paramater)
          "blank_log_prob": {"class": "eval", "from": "emit_prob0", "eval": "tf_compat.v1.log_sigmoid(-source(0))"},

          # a. Distribution over vocabulary w/o blank
          "label_emit_log_prob": {"class": "combine", "kind": "add", "from": ["label_log_prob", "emit_log_prob"],
            "n_out": _target_num_labels,
            "out_type": {"dim": _target_num_labels}},  # 1 gets broadcasted
          # b. Distribution over the whole vocabulary inclusive blank (concatinate a. & b.)
          "output_log_prob": {"class": "copy", "from": ["label_emit_log_prob", "blank_log_prob"],
            "is_output_layer": full_sum_alignment},
        }
        if ce_loss:
          voc_dict["output_prob"] = {
            "class": "activation", "from": "output_log_prob", "activation": "exp",
            "target": target, "loss": "ce", "loss_opts": {"focal_loss_factor": 2.0}
          }
        elif full_sum:
          voc_dict["emit_prob0"]["from"] = "readout"  # not "s"
          voc_dict["output_prob"] = {
            "class": "eval",
            "from": ["output_log_prob", "base:data:" + _target, "base:encoder"],
            "eval": rna_loss,
            "out_type": lambda sources, **kwargs: Data(name="rnna_loss", shape=()), "loss": "as_is",
          }
        return voc_dict

      # 5.
      def get_new_input_dict():
        """
        Requires
                "output_": output after switchout
        Provides
                "output_emit": True if new input is not blank. Starts with True.
        """
        return {
          ## Check if output was blank
          "output_is_not_blank": {"class": "compare", "from": "output_", "value": _targetb_blank_idx,
            "kind": "not_equal", "initial_output": True},
          # This "output_emit" is True on the first label but True otherwise, and False on blank.
          "output_emit": {"class": "copy", "from": "output_is_not_blank", "initial_output": True,
            "is_output_layer": True}
        }

      output_dict = {
        "class": "rec",
        "from": "encoder",
        "include_eos": True,
        "back_prop": train,
        "unit": {},
        "size_target": target if ce_loss and train else None,  # Viterbi training, uses a more powerful state-layer
        # "max_seq_len": "max_len_from('base:encoder') * 2",
      }
      unit = output_dict["unit"]

      # 0 Output of encoder serving as query
      # <- "encoder"
      unit["am"] = {"class": "copy",
        "from": "data:source"}  # data:source is the input of the rec_layer, i.e input of decoder
      # 1. ATTENTION (self attention on encoders output)
      # <- "am"=="encoder": query
      # <- "base:enc_ctx_win": keys from encoder
      # <- "base:enc_val_win": values from encoder
      # -> "att": attention of "am"
      unit.update(get_attention_dict())  # provides "att"
      # 2. SLOW RNN
      # <- "prev:output_emit": True if last emit was not blank. Starts with True. (True/False)
      # <- "prev:output_": the last output. In case of training, after switchout (sparse)
      # <- "base:lm_input": target sequence with blank prepended (sparse)
      # <- "base:att": SelfAttention on the Encoder's output (Vector)
      # -> "lm": SlowRnn output
      unit.update(get_decoder_slow_rnn_dict())
      # 3. FAST RNN
      # <- "lm": SlowRnn output
      # -> "s": state of the decoder
      # -> "label_log_prob": Log Distribution over vocab w/o blank
      unit.update(get_decoder_fast_rnn_cell_dict())
      # 4. EMITING Distributions
      # <- "label_log_prob": Log Distribution over vocab w/o blank
      # <- "s": State of LSTM Cell
      # -> "output_log_prob": the log distribution over the whole vocab inkl blank
      unit.update(get_vocab_and_blank_distro_dict())
      # 5. OUTPUT CHOICE
      # <- "output_log_prob": the log distribution over the whole vocab inkl blank
      # <- "target": target - makes sure that we can cheat during training(i.e "bpe" for FS and "targetb" for CE)
      # -> "output": [B, beam_size, 1] for each batch return "beam_size" one hot encoded(as indexes thats why 1) choices
      unit["output"] = {
        "class": 'choice', 'target': target, 'beam_size': beam_size,
        "from": "output_log_prob", "input_type": "log_prob",
        "initial_output": 0,
        "cheating": "exclusive" if train else None,
        "explicit_search_sources": ["prev:out_str", "prev:output"] if task == "search" else None,  # ??
        "custom_score_combine": targetb_recomb_recog if task == "search" else None}  # ??
      # 6. OUTPUT STRING
      # <- "prev:out_str", "output_emit", "output" : if "output_emit" add "output" to "out_str"
      # -> "out_str": string up to time point
      unit["out_str"] = {
        "class": "eval", "from": ["prev:out_str", "output_emit", "output"],
        "initial_output": None, "out_type": {"shape": (), "dtype": "string"},
        "eval": out_str}  # only for debug/search
      # 7. SWITCHOUT
      # <- "output": hot encoded choices
      # -> "output_": output after switchout (sparse)
      if ce_loss and train:
        unit["output_"] = {"class": "eval", "from": "output", "eval": switchout_target,
          "initial_output": 0}  # switchout only applicable to viterbi training,
      else:
        unit["output_"] = {"class": "copy", "from": "output", "initial_output": 0}
      # 8. NEW INPUT? BOOL
      # <- "output_": output after switchout
      # -> "output_emit":True if new input is not blank. Starts with True.
      unit.update(get_new_input_dict())
      return output_dict

    net_dict = {"#config": {}}
    # 1. Add input [x]
    net_dict.update(get_input_dict())
    # 2. Add encoder stack [x]
    net_dict.update(get_encoder_dict())
    # 3. Add LM Inputs. IF FULL_SUM (dependent on data:targets) [x]
    if not ce_loss: net_dict.update(get_lm_inputs_dict())  # only for full_sum, for ce_loss forced alignements are used
    # 4. Add decoder(slowrnn + fastrnn + beamsearch) (dependent on encoder's output and LM input)
    net_dict["output"] = get_output_dict(train=(task == "train"), search=(task != "train"), target=target,
      beam_size=beam_size)
    # 5. Add CTC_outputs (dependent on encoder's output & decoder's output) [x]
    # <- "output_emit":True if new input is not blank. Starts with True.
    if not full_sum: net_dict.update(get_ctc_dict())
    # 6. Add output decision (ONLY in Search) [x]
    # <- "output_emit":True if new input is not blank. Starts with True.
    # -> "decision": the decision taken while searching
    net_dict.update(get_output_decision_dict())
    return net_dict

  @classmethod
  def _get_network_full_sum(cls, pretrain_frac: int):
    net_dict = cls._get_network(full_sum_loss=True, pretrain_frac=pretrain_frac,
      target="classes" if task == "train" else "targetb")
    return net_dict

  @classmethod
  def _get_network_align(cls, epoch0: int):
    """
    This is the network used to create the alignments.
    In addition to the FS network it calculates the alignments and writes them in a .hdf file.
    """
    net_dict = cls._get_network(full_sum_alignment=True, target="bpe" if task == "train" else "targetb")

    subnet = net_dict["output"]["unit"]
    subnet["fullsum_alignment"] = {
      "class": "eval",
      "from": ["output_log_prob", "base:data:" + _target, "base:encoder"],
      "eval": rna_fullsum_alignment,
      "out_type": lambda sources, **kwargs: Data(name="rna_alignment_output", sparse=True, dim=_targetb_num_labels,
        size_placeholder={0: sources[2].output.size_placeholder[0]}),
      "is_output_layer": True
    }
    align_dir = os.path.dirname(model)
    subnet["_align_dump"] = {
      "class": "hdf_dump",
      "from": "fullsum_alignment",
      "is_output_layer": True,
      "dump_per_run": True,
      "extend_existing_file": epoch0 % EpochSplit > 0,
      "filename": (lambda **opts: "%s/align.{dataset_name}.hdf".format(**opts) % align_dir),
    }
    return net_dict

  @classmethod
  def _get_network_default(cls, pretrain_frac):
    """
    The default network is the network trained with CE.
    In addition to the other datasets it uses "alignments"
    which are represented as extern_data under the name "targetb".
    """
    net_dict = cls._get_network(ce_loss=True, target="targetb", pretrain_frac=pretrain_frac)
    net_dict.update({
      "existing_alignment": {
        "class": "reinterpret_data", "from": "data:alignment",
        "set_sparse": True,  # not sure what the HDF gives us
        "set_sparse_dim": _targetb_num_labels,
        "size_base": "encoder",  # for RNA...
      },
      # The layer name must be smaller than "t_target" such that this is created first.
      "1_targetb_base": {
        "class": "copy",
        "from": "existing_alignment",
        "register_as_extern_data": "targetb" if task == "train" else None},
    })

    return net_dict


class Trainer():
  """ Global Parameters Required: learning_rate """
  # E.g. for
  # Our training pipeline looks as follows:
  ## Full-sum training: 0->25 Epochs(0->150 SubEpochs)
  ### First we start with a small encoder which we grow over 20 SubEpochs, while warming up the LR.
  ## Alignment generation: 25->26 Epochs (150->156 SubEpochs)
  ### After 150 SubEpochs of training, we generate the best path alignments
  ## Viterbi training: 26->76=50 Epochs (156->456 Subepochs)
  ### For 150 SubEpochs normal training (growing the encoder, more complex decoder(LSTM)) while increasing the LR
  ### Then LR reset and train again (using the same network params). We warmup the LR again also.

  ## Fullsum + Alignment-gen + Viterbi(CE) : 150 + 6 + 300 (SubEpochs)
  ### FS: first 25 Epochs (150 SubEpochs)
  ### Alignment: the 26th Epoch (6 SubEpochs)
  ### CE: the rest 50 Epochs (300 SubEpochs)
  _range_epochs_full_sum = (0, EpochSplit * 25)  # 0->150 SubEpochs ~ 0->25 Epoch
  _range_epochs_align = (
    _range_epochs_full_sum[1], _range_epochs_full_sum[1] + EpochSplit)  # 150->156 SubEpochs ~ 25->26 Epoch
  _lr_reset = _range_epochs_align[1]  # 156 SubEpoch ~ 26 Epoch (reset LR the first Epoch)
  ## Pretraining for FS and Viterbi(CE)
  ## FS: pretrain the first 20 SubEpochs
  ## CE: FIRST 150 SubEpochs(25Epochs): pretrain for 20 SubEpochs, Train for the rest 130 SubEpochs,
  ##     LAST  150 SubEpochs(25Epochs): pretrain for again 20 SubEpochs, train normally for the rest(use the same weights)
  _pretrain_warmup_lr_frac = 0.5  # warmup lr up to this frac(happens only during pretraining)
  _pretrain_epochs = 20
  _range_epochs_pretrain_fullsum = (0, _pretrain_epochs)
  _range_epochs_pretrain_vit = [
    (_range_epochs_align[1], _range_epochs_align[1] + _pretrain_epochs),
    (_range_epochs_align[1] + EpochSplit * 25, _range_epochs_align[1] + EpochSplit * 25 + _pretrain_epochs)
  ]
  _eval_datasets = ["devtrain"]  # possible evaluation datasets used during CE

  @classmethod
  def _get_viterbi_pretrain_frac(cls, epoch0):
    """ Viterbi Pretrain Fraction: increase linearly(0 -> 1) during each interval in _range_epochs_pretrain_vit """
    pretrain_frac = 0
    for sched_start, sched_end in cls._range_epochs_pretrain_vit:
      if epoch0 < sched_start:  # pretraining=0 if still FS training or aligning
        continue
      if epoch0 < sched_end:  #
        pretrain_frac = (float(epoch0 - sched_start) / (sched_end - sched_start))
      else:
        pretrain_frac = 1
    return pretrain_frac

  @classmethod
  def _get_fullsum_pretrain_frac(cls, epoch0):
    """ FS Pretrain Fraction: increase frac linearly(0 -> 1) according to interval _range_epochs_pretrain_fullsum"""
    if epoch0 < cls._range_epochs_pretrain_fullsum[0]:
      pretrain_frac = 0
    elif epoch0 < cls._range_epochs_pretrain_fullsum[1]:
      pretrain_frac = (
          float(epoch0 - cls._range_epochs_pretrain_fullsum[0]) /
          (cls._range_epochs_pretrain_fullsum[1] - cls._range_epochs_pretrain_fullsum[0]))
    else:
      pretrain_frac = 1
    return pretrain_frac

  @classmethod
  def _get_lr(cls, pretrain_frac):
    ## Warmup during 0 < pre_frac < 0.5: 0.1*lr -> 0.6*lr and then 1??
    lr = None
    if pretrain_frac < cls._pretrain_warmup_lr_frac:
      start_lr = learning_rate / 10.
      # lr = start_lr + pretrain_frac * (_learning_rate - _min_learning_rate)
      lr = start_lr + (1 / cls._pretrain_warmup_lr_frac) * pretrain_frac * learning_rate
    elif pretrain_frac < 1:
      lr = learning_rate
    else:
      assert (False)  ## should not enter here
    return lr

  @classmethod
  def _get_network_full_sum(cls, epoch0: int):
    pretrain_frac = cls._get_fullsum_pretrain_frac(epoch0)
    net_dict = Network._get_network_full_sum(pretrain_frac=pretrain_frac)
    if pretrain_frac < 1:
      net_dict["#config"]["learning_rate"] = cls._get_lr(pretrain_frac)

    # Fullsum loss requires way more memory
    net_dict["#copy_param_mode"] = "subset"
    net_dict["#config"]["batch_size"] = 4000  # rna-tf2.blank0.rdrop best FS part
    net_dict["#config"]["accum_grad_multiple_step"] = 3
    return net_dict

  @classmethod
  def _get_network_align(cls, epoch0: int):
    net_dict = Network._get_network_align(epoch0)
    net_dict["#trainable"] = False  # disable training
    net_dict["#finish_all_data"] = True  # in case of multi-GPU training or so
    return net_dict

  @classmethod
  def _get_network_default(cls, epoch0: int, reset: bool = False):
    pretrain_frac = cls._get_viterbi_pretrain_frac(epoch0)
    net_dict = Network._get_network_default(pretrain_frac=pretrain_frac)
    if pretrain_frac < 1:
      net_dict["#config"]["learning_rate"] = cls._get_lr(pretrain_frac)

    if reset:
      net_dict["#copy_param_mode"] = "reset"

    # Chunking
    _time_red = 6
    _chunk_size = 60
    net_dict["#config"].update({
      # ..., TODO more? e.g. maximize GPU mem util
      "chunking":  # can use chunking with frame-wise training
        ({"data": _chunk_size * _time_red, "alignment": _chunk_size},
        {"data": _chunk_size * _time_red // 2, "alignment": _chunk_size // 2})
    })

    # New Alignement Datasets
    align_dir = os.path.dirname(model)
    net_dict["#config"]["train"] = Dataset.get_aligned_hdf_dataset("train", "%s/align.train.hdf" % align_dir)
    net_dict["#config"]["dev"] = Dataset.get_aligned_hdf_dataset("cv", "%s/align.dev.hdf" % align_dir)
    net_dict["#config"]["eval_datasets"] = {
      key: Dataset.get_aligned_hdf_dataset(key, "%s/align.%s.hdf" % (align_dir, key)) for key in cls._eval_datasets}
    return net_dict

  @classmethod
  def get_network(cls, epoch: int, ):
    epoch0 = epoch - 1  # RETURNN starts with epoch 1, but 0-indexed is easier here
    if cls._range_epochs_full_sum[0] <= epoch0 < cls._range_epochs_full_sum[1]:  # 0->25 Epochs(0->150 SubEpochs)
      print("Epoch %i: Constructing network using full-sum, pretrain_frac=%.1f" % (
        epoch0 + 1, cls._get_fullsum_pretrain_frac(epoch0)))
      return cls._get_network_full_sum(epoch0=epoch0)
    elif cls._range_epochs_align[0] <= epoch0 < cls._range_epochs_align[1]:  # 25->26 Epochs (150->156 SubEpochs)
      print("Epoch %i: Constructing network using full-sum alignment generation." % epoch)
      return cls._get_network_align(epoch0=epoch0)
    elif epoch0 == _lr_reset:  # 26 Epoch (156 SubEpoch): reset network
      print("Epoch %i: Constructing network using Viterbi training, resetting, pretrain_frac=%.1f." % (
        epoch, cls._get_viterbi_pretrain_frac(epoch0)))
      return cls._get_network_default(epoch0=epoch0, reset=True)
    else:  # 26->76=50 Epochs (156->456 Subepochs)
      print("Epoch %i: Constructing network using Viterbi training, default training, pretrain_frac=%.1f." % (
        epoch, cls._get_viterbi_pretrain_frac(epoch0)))
      return cls._get_network_default(epoch0=epoch0)


#########################################################
############ B. Dataset- & Network- Creation ############
get_network = Trainer.get_network
### Switchboard
# # Datasets
# train = Dataset.get_sprint_dataset("train")
# dev = Dataset.get_sprint_dataset("cv")
# eval_datasets = {"devtrain": Dataset.get_sprint_dataset("devtrain")}

# # Input/Output of Network
# extern_data = Dataset.get_extern_data(task, _target_num_labels, _targetb_num_labels)
### a) Input: Batch, seq_1_len, 40
### b) Target: Batch, seq_t_len, 1030
### c) Alignement: Barch, seq_a_len, 1031
## Forced Alignement
### d) TargetB: Batchm seq_ta_len, 1031

### Librispeech
# Datasets
train = Dataset.get_librispeech_dataset("train", train_partition_epoch=EpochSplit)
dev = Dataset.get_librispeech_dataset("dev", subset=3000)
eval_datasets = {"devtrain": Dataset.get_librispeech_dataset("train", subset=2000)}

# Input/Output of Network
extern_data = Dataset.get_librispeech_extern_data(task, _target_num_labels, _targetb_num_labels)
### a) Input: Batch, seq_1_len, 40
### b) Target: Batch, seq_t_len, 1030
### c) Alignement: Barch, seq_a_len, 1031
## Forced Alignement
### d) TargetB: Batchm seq_ta_len, 1031


####################################################################
############### C. CONFIGURATIONS NOT REQUIRED ABOVE ###############
# 1. Training Information
# num_epochs = 150 + 6 + 300  # Fullsum + Alignment-gen + Viterbi (SubEpochs)
num_epochs = 12
cleanup_old_models = True

# 2. Output Information
search_output_layer = "decision"
debug_print_layer_output_template = True

# 3. Batching Information
batching = "random"
log_batch_size = True
batch_size = 10000
max_seqs = 200
max_seq_length = 0  # {"bpe": 75}
chunking = None  # only used in viterbi training # chunking_variance ... # min_chunk_size ...

# 4. Optimization Information
gradient_clip = 0
# gradient_clip_global_norm = 1.0
adam = True
optimizer_epsilon = 1e-8
accum_grad_multiple_step = 2
# debug_add_check_numerics_ops = True
# debug_add_check_numerics_on_output = True
stop_on_nonfinite_train_score = False
tf_log_memory_usage = True
gradient_noise = 0.0

# 5. Learning Rate Schedule Information
learning_rate_control = "newbob_multi_epoch"
learning_rate_control_error_measure = "dev_error_output/output_prob"
learning_rate_control_relative_error_relative_lr = True
learning_rate_control_min_num_epochs_per_new_lr = 3
use_learning_rate_control_always = True
newbob_multi_num_epochs = 6
newbob_multi_update_interval = 1
newbob_learning_rate_decay = 0.7
learning_rate_file = "newbob.data"

# 6. Log
# log = "| /u/zeyer/dotfiles/system-tools/bin/mt-cat.py >> log/crnn.seq-train.%s.log" % task
log = "log/crnn.%s.log" % task
log_verbosity = 5
##############################################################################################
##############################################################################################
