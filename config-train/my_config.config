#!returnn.py
# -*- mode: python -*-
# vim:set expandtab tabstop=4 fenc=utf-8 ff=unix ft=python:

import os
from pathlib import Path
from returnn.import_ import import_
import_("github.com/jotix16/returnn_modules", "zhobro_2021", None)
from returnn_import.github_com.jotix16.returnn_modules.dev.zhobro_2021.dataset.LibriSpeech.libri import Dataset
from returnn_import.github_com.jotix16.returnn_modules.dev.zhobro_2021.pretrain.pretrain import Trainer
from returnn_import.github_com.jotix16.returnn_modules.dev.zhobro_2021.common_config import *


globals().update(a=2)

############ B. Dataset- & Network- Creation ############
MyTrainer = Trainer("train")
get_network = MyTrainer.get_network

### Switchboard
# # Datasets
# train = Dataset.get_sprint_dataset("train")
# dev = Dataset.get_sprint_dataset("cv")
# eval_datasets = {"devtrain": Dataset.get_sprint_dataset("devtrain")}

# # Input/Output of Network
# extern_data = Dataset.get_extern_data(task, _target_num_labels, _targetb_num_labels)
### a) Input: Batch, seq_1_len, 40
### b) Target: Batch, seq_t_len, 1030
### c) Alignement: Barch, seq_a_len, 1031
## Forced Alignement
### d) TargetB: Batchm seq_ta_len, 1031

### Librispeech
# Datasets

dataset_path = os.path.join(Path(os.path.dirname(os.path.abspath(__file__))).parent.absolute(), "base/dataset_symlink")
LibrispeechDataset = Dataset(dataset_path, _target_num_labels, _targetb_num_labels)
train, dev, eval_datasets = LibrispeechDataset.get_dataset()
extern_data = LibrispeechDataset.get_librispeech_extern_data(task)

### a) Input: Batch, seq_1_len, 40
### b) Target: Batch, seq_t_len, 1030
### c) Alignement: Barch, seq_a_len, 1031
## Forced Alignement
### d) TargetB: Batchm seq_ta_len, 1031


####################################################################
############### C. CONFIGURATIONS NOT REQUIRED ABOVE ###############
# 1. Training Information
# num_epochs = 150 + 6 + 300  # Fullsum + Alignment-gen + Viterbi (SubEpochs)
num_epochs = 12
cleanup_old_models = True

# 2. Output Information
search_output_layer = "decision"
debug_print_layer_output_template = True

# 3. Batching Information
batching = "random"
log_batch_size = True
batch_size = 10000
max_seqs = 200
max_seq_length = 0  # {"bpe": 75}
chunking = None  # only used in viterbi training # chunking_variance ... # min_chunk_size ...

# 4. Optimization Information
gradient_clip = 0
# gradient_clip_global_norm = 1.0
adam = True
optimizer_epsilon = 1e-8
accum_grad_multiple_step = 2
# debug_add_check_numerics_ops = True
# debug_add_check_numerics_on_output = True
stop_on_nonfinite_train_score = False
tf_log_memory_usage = True
gradient_noise = 0.0

# 5. Learning Rate Schedule Information
learning_rate_control = "newbob_multi_epoch"
learning_rate_control_error_measure = "dev_error_output/output_prob"
learning_rate_control_relative_error_relative_lr = True
learning_rate_control_min_num_epochs_per_new_lr = 3
use_learning_rate_control_always = True
newbob_multi_num_epochs = 6
newbob_multi_update_interval = 1
newbob_learning_rate_decay = 0.7
learning_rate_file = "newbob.data"

# Environment
use_tensorflow = True
# device = "gpu"
device = "cpu"
multiprocessing = True
update_on_device = True
cache_size = "0"
window = 1

# 6. Log
# log = "| /u/zeyer/dotfiles/system-tools/bin/mt-cat.py >> log/crnn.seq-train.%s.log" % task
log = "log/crnn.%s.log" % task
log_verbosity = 5
##############################################################################################
##############################################################################################
